{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bIsjZSqSgmhz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# USING THE Training data generated from a ANN-DQN and trying to predict what action to take based on the image that is 4 images stacked "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8cP6jwQmgmh1"
      },
      "outputs": [],
      "source": [
        "# General libraries\n",
        "import pickle\n",
        "#For model building\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ARVfYf55gmh2"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "from gym import wrappers\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import imageio\n",
        "import numpy as np\n",
        "from skimage.transform import resize\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import torchvision.transforms as transforms\n",
        "import tensorflow.compat.v1 as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "fLvG0t9Igmh2"
      },
      "outputs": [],
      "source": [
        "class Atari(object):\n",
        "    \"\"\"Wrapper for the environment provided by gym\"\"\"\n",
        "    def __init__(self, envName, no_op_steps=10, agent_history_length=4):\n",
        "        self.env = gym.make(envName)\n",
        "        self.state = None\n",
        "        self.last_lives = 0\n",
        "        self.no_op_steps = no_op_steps\n",
        "        self.agent_history_length = agent_history_length\n",
        "\n",
        "    def reset(self,evaluation=False):\n",
        "\n",
        "        frame = self.env.reset()\n",
        "        self.last_lives = 0\n",
        "        terminal_life_lost = True # Set to true so that the agent starts \n",
        "                                  # with a 'FIRE' action when evaluating\n",
        "        if evaluation:\n",
        "            for _ in range(random.randint(1, self.no_op_steps)):\n",
        "                frame, _, _, _ = self.env.step(1) # Action 'Fire'\n",
        "        processed_frame = frameprocess(frame)   # (★★★)\n",
        "        self.state = np.repeat(processed_frame, self.agent_history_length, axis=2)\n",
        "        \n",
        "        return terminal_life_lost\n",
        "\n",
        "    def step(self,action):\n",
        "\n",
        "        new_frame, reward, terminal, info = self.env.step(action)  # (5★)\n",
        "            \n",
        "        if info['lives'] < self.last_lives:\n",
        "            terminal_life_lost = True\n",
        "        else:\n",
        "            terminal_life_lost = terminal\n",
        "        self.last_lives = info['lives']\n",
        "        \n",
        "        processed_new_frame = frameprocess(new_frame)   # (6★)\n",
        "        new_state = np.append(self.state[:, :, 1:], processed_new_frame, axis=2) # (6★)   \n",
        "        self.state = new_state\n",
        "        \n",
        "        return processed_new_frame, reward, terminal, terminal_life_lost, new_frame"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZDt-GtVgmh3"
      },
      "source": [
        "# Loads the training data and cleans it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44waaOOdgmh5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "id": "uFSzODTAgmh5",
        "outputId": "aa66ab35-d0b3-4ce0-d7b1-8a4e3012b670"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'pickeledDataPong'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\abdel\\Reinforcement-Learning-on-Spiking-Neural-Networks\\CNN-Trained-Using-a-Pretrained-ANN\\CNNTARI.ipynb Cell 8'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/abdel/Reinforcement-Learning-on-Spiking-Neural-Networks/CNN-Trained-Using-a-Pretrained-ANN/CNNTARI.ipynb#ch0000007?line=0'>1</a>\u001b[0m totensor \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39mToTensor()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/abdel/Reinforcement-Learning-on-Spiking-Neural-Networks/CNN-Trained-Using-a-Pretrained-ANN/CNNTARI.ipynb#ch0000007?line=2'>3</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(\u001b[39m'\u001b[39;49m\u001b[39mpickeledDataPong\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/abdel/Reinforcement-Learning-on-Spiking-Neural-Networks/CNN-Trained-Using-a-Pretrained-ANN/CNNTARI.ipynb#ch0000007?line=3'>4</a>\u001b[0m     trainingData \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39mload(f)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/abdel/Reinforcement-Learning-on-Spiking-Neural-Networks/CNN-Trained-Using-a-Pretrained-ANN/CNNTARI.ipynb#ch0000007?line=4'>5</a>\u001b[0m \u001b[39m# Reformat training data to predict the next 20 moves\u001b[39;00m\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'pickeledDataPong'"
          ]
        }
      ],
      "source": [
        "totensor = transforms.ToTensor()\n",
        "\n",
        "with open('pickeledDataPong', 'rb') as f:\n",
        "    trainingData = pickle.load(f)\n",
        "# Reformat training data to predict the next 20 moves\n",
        "trainset,testset = train_test_split(trainingData,test_size=0.3)\n",
        "l = len(trainset)\n",
        "trainimages = []\n",
        "trainlabels = []\n",
        "testimages = []\n",
        "testlabels = []\n",
        "for i in range(l):\n",
        "    if trainset[i][1] not in [0,1,2]:\n",
        "        trainimages.append(totensor(np.array(trainset[i][0])[:,14:66]).cpu().detach().numpy())\n",
        "        trainlabels.append(np.array([k==trainset[i][1] for k in range(6)]))\n",
        "lt = len(testset)\n",
        "for i in range(lt):\n",
        "    if testset[i][1] not in [0,1,2]:\n",
        "        testimages.append(totensor(np.array(testset[i][0])[:,14:66]).cpu().detach().numpy())\n",
        "        testlabels.append(testset[i][1])\n",
        "\n",
        "\n",
        "\n",
        "train_data = TensorDataset(torch.Tensor(np.array(trainimages)), torch.Tensor(np.array(trainlabels)))\n",
        "test_data = TensorDataset(torch.Tensor(np.array(testimages)),torch.Tensor(np.array(testlabels)))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "A65FQBfugmh6"
      },
      "outputs": [],
      "source": [
        "def frameprocess(frame,frame_height=84, frame_width=65):\n",
        "\n",
        "    frame_height = frame_height\n",
        "    frame_width = frame_width\n",
        "    processed = tf.image.rgb_to_grayscale(frame)\n",
        "    processed = tf.image.crop_to_bounding_box(processed, 34, 0, 160, 140)\n",
        "    processed = tf.image.resize_images(processed, \n",
        "                                            [frame_height, frame_width], \n",
        "                                            method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "    return processed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "_VIV5l7fgmh7"
      },
      "outputs": [],
      "source": [
        "def generate_gif(frames_for_gif):\n",
        "\n",
        "    for idx, frame_idx in enumerate(frames_for_gif): \n",
        "        frames_for_gif[idx] = resize(frame_idx, (420, 320, 3), \n",
        "                                     preserve_range=True, order=0).astype(np.uint8)\n",
        "        \n",
        "    imageio.mimsave(\"ATARI_PONG.gif\", \n",
        "                    frames_for_gif, duration=1/30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BtyyUwKWgmh8"
      },
      "outputs": [],
      "source": [
        "#newTrainingData[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-er9Fvpgmh8"
      },
      "outputs": [],
      "source": [
        "\"\"\"# import required libraries\n",
        "import torch\n",
        "import torchvision.transforms as T\n",
        "from PIL import Image\n",
        "\n",
        "# define a torch tensor\n",
        "print(trainingData[1000][0].shape)\n",
        "tensor = newTrainingData[1000][0]\n",
        "\n",
        "# define a transform to convert a tensor to PIL image\n",
        "transform = T.ToPILImage()\n",
        "nd = transforms.ToTensor()\n",
        "# convert the tensor to PIL image using above transform\n",
        "img = transform(tensor)\n",
        "imgten = nd(newTrainingData[1000][0])\n",
        "print(imgten.shape)\n",
        "# display the PIL image\n",
        "img.show()\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "_t2xWC7-gmh9"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'train_data' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\abdel\\Reinforcement-Learning-on-Spiking-Neural-Networks\\CNN-Trained-Using-a-Pretrained-ANN\\CNNTARI.ipynb Cell 13'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/abdel/Reinforcement-Learning-on-Spiking-Neural-Networks/CNN-Trained-Using-a-Pretrained-ANN/CNNTARI.ipynb#ch0000012?line=0'>1</a>\u001b[0m \u001b[39mlen\u001b[39m(train_data)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/abdel/Reinforcement-Learning-on-Spiking-Neural-Networks/CNN-Trained-Using-a-Pretrained-ANN/CNNTARI.ipynb#ch0000012?line=1'>2</a>\u001b[0m batch_size \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'train_data' is not defined"
          ]
        }
      ],
      "source": [
        "len(train_data)\n",
        "batch_size = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MG4gqvs8gmh9"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Dq7gatb_gmh-"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'train_data' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\abdel\\Reinforcement-Learning-on-Spiking-Neural-Networks\\CNN-Trained-Using-a-Pretrained-ANN\\CNNTARI.ipynb Cell 15'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/abdel/Reinforcement-Learning-on-Spiking-Neural-Networks/CNN-Trained-Using-a-Pretrained-ANN/CNNTARI.ipynb#ch0000014?line=0'>1</a>\u001b[0m batch_size \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/abdel/Reinforcement-Learning-on-Spiking-Neural-Networks/CNN-Trained-Using-a-Pretrained-ANN/CNNTARI.ipynb#ch0000014?line=2'>3</a>\u001b[0m trainloader \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataLoader(train_data, batch_size\u001b[39m=\u001b[39mbatch_size,shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,drop_last\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, num_workers\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/abdel/Reinforcement-Learning-on-Spiking-Neural-Networks/CNN-Trained-Using-a-Pretrained-ANN/CNNTARI.ipynb#ch0000014?line=3'>4</a>\u001b[0m testloader \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataLoader(test_data, batch_size\u001b[39m=\u001b[39mbatch_size,shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,drop_last\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,num_workers\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'train_data' is not defined"
          ]
        }
      ],
      "source": [
        "batch_size = 1\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,shuffle=True,drop_last=True, num_workers=2)\n",
        "testloader = torch.utils.data.DataLoader(test_data, batch_size=batch_size,shuffle=True,drop_last=True,num_workers=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIVmlzWogmh-"
      },
      "outputs": [],
      "source": [
        "\"\"\" for image,label in trainloader:\n",
        "    # define a torch tensor\n",
        "    print(image[0].shape)\n",
        "    tensor = image[0]\n",
        "\n",
        "    # define a transform to convert a tensor to PIL image\n",
        "    transform = T.ToPILImage()\n",
        "\n",
        "    # convert the tensor to PIL image using above transform\n",
        "    img = transform(tensor)\n",
        "\n",
        "    # display the PIL image\n",
        "    img.show()\n",
        "    break\"\"\"\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "8qd-xK89gmh-"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "batch_size = 1\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(4, 6,8,4)\n",
        "        self.conv2 = nn.Conv2d(6, 16,4,2)\n",
        "        self.conv3 = nn.Conv2d(16,32,3,1)\n",
        "        self.conv4 = nn.Conv2d(32,64,7,1)\n",
        "        \n",
        "        self.fc1 = nn.Linear(720*batch_size, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fclast = nn.Linear(84, 6*batch_size)\n",
        "        self.softmax = torch.nn.Softmax()\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        \n",
        "        x = torch.flatten(x) # flatten all dimensions\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))    \n",
        "        x =self.fclast(x)    \n",
        "        sig_out = x.view(batch_size, -1)\n",
        "        #print(x)\n",
        "        sig_out = self.softmax(sig_out)\n",
        "        #print(\"sig x\",sig_out)\n",
        "        #print(sig_out)\n",
        "        return sig_out\n",
        "\n",
        "\n",
        "net = Net()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=================================================================\n",
            "Layer (type:depth-idx)                   Param #\n",
            "=================================================================\n",
            "├─Conv2d: 1-1                            1,542\n",
            "├─Conv2d: 1-2                            1,552\n",
            "├─Conv2d: 1-3                            4,640\n",
            "├─Conv2d: 1-4                            100,416\n",
            "├─Linear: 1-5                            86,520\n",
            "├─Linear: 1-6                            10,164\n",
            "├─Linear: 1-7                            510\n",
            "├─Softmax: 1-8                           --\n",
            "=================================================================\n",
            "Total params: 205,344\n",
            "Trainable params: 205,344\n",
            "Non-trainable params: 0\n",
            "=================================================================\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "=================================================================\n",
              "Layer (type:depth-idx)                   Param #\n",
              "=================================================================\n",
              "├─Conv2d: 1-1                            1,542\n",
              "├─Conv2d: 1-2                            1,552\n",
              "├─Conv2d: 1-3                            4,640\n",
              "├─Conv2d: 1-4                            100,416\n",
              "├─Linear: 1-5                            86,520\n",
              "├─Linear: 1-6                            10,164\n",
              "├─Linear: 1-7                            510\n",
              "├─Softmax: 1-8                           --\n",
              "=================================================================\n",
              "Total params: 205,344\n",
              "Trainable params: 205,344\n",
              "Non-trainable params: 0\n",
              "================================================================="
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#%pip install torchinfo \n",
        "import torchsummary\n",
        "batch_size = 16\n",
        "torchsummary.summary(net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EaRditSrgmh-"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBlGJURJgmh-"
      },
      "outputs": [],
      "source": [
        "# zero the parameter gradients\n",
        "optimizer.zero_grad()\n",
        "# forward + backward + optimize\n",
        "#print(torch.tensor([input]).size())\n",
        "for epoch in range(1):\n",
        "    for image,move in trainloader:\n",
        "        outputs = net(image)\n",
        "        #print(torch.tensor(outputs))\n",
        "        #print(move)\n",
        "        #print(torch.argmax(outputs),torch.tensor(move))\n",
        "        loss = criterion(outputs,torch.tensor(move))\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uV7W1_qugmh_"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "# zero the parameter gradients\n",
        "optimizer.zero_grad()\n",
        "\n",
        "for input,labels in trainloader:\n",
        "    # forward + backward + optimize\n",
        "    #print(torch.tensor([input]).size())\n",
        "    outputs = net(torch.tensor(input))\n",
        "    loss = sum([criterion(output, l) for (l, output) in zip(outputs, labels)])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zEe2iZDogmh_"
      },
      "outputs": [],
      "source": [
        "def check_accuracy(outputs,target):\n",
        "    #print(outputs)\n",
        "    return np.equal(np.array([torch.argmax(outputs)]),np.array(target))\n",
        "\n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eWxpQmcUgmh_"
      },
      "outputs": [],
      "source": [
        "accuracy = []\n",
        "for image,moves in testloader:\n",
        "    outputs = net(image)\n",
        "    #print(check_accuracy(outputs,moves))\n",
        "    accuracy.append(check_accuracy(outputs,moves))\n",
        "    #print(torch.argmax(outputs))\n",
        "\n",
        "print(np.mean(accuracy))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-MdjYQeUgmh_"
      },
      "outputs": [],
      "source": [
        "torch.save(net.state_dict(), \"savedmodel\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-i4VgFgCgmh_"
      },
      "outputs": [],
      "source": [
        "torch.save(net, \"FULLMODEL\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19OUJdw4gmiA"
      },
      "outputs": [],
      "source": [
        "#2up 3 down 4 up 5 down"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lqt2XwoEgmiA"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7BGDjlpgmiA"
      },
      "outputs": [],
      "source": [
        "#tt = transforms.ToTensor()\n",
        "env = Atari(\"PongDeterministic-v4\")\n",
        "env.reset()\n",
        "frames=[]\n",
        "total_reward=0\n",
        "done = False\n",
        "env.reset()\n",
        "i=0\n",
        "\n",
        "for i in range(300):\n",
        "    action = 2\n",
        "    observation, reward, done, info,new_frame = env.step(action)\n",
        "    frames.append(new_frame)\n",
        "    env.env.render()\n",
        "#transform = transforms.ToPILImage()\n",
        "\n",
        "# convert the tensor to PIL image using above transform\n",
        "#img = transform(tt(env.state))\n",
        "\n",
        "# display the PIL image\n",
        "#img.save(\"PIL.png\")\n",
        "    \n",
        "while not done:\n",
        "    i+=1\n",
        "    #s = env.state\n",
        "    action = net(torch.Tensor(np.array([totensor(env.state).cpu().detach().numpy() for i in range(batch_size)])))\n",
        "    #print(\"action\",action)\n",
        "    #env.step(action)\n",
        "    #print(torch.argmax(action))\n",
        "    observation, reward, done, info,new_frame = env.step(int(torch.argmax(action[0])))\n",
        "    #(env.state).show()\n",
        "    \n",
        "    frames.append(new_frame)\n",
        "    env.env.render()\n",
        "\n",
        "print(i)\n",
        "env.reset()\n",
        "generate_gif(frames)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Idea Abandoned As 40% accuracy seems too bad\n",
        "# Was planning on converting to SNN but not worth it as some accuarcy will be lost in the process\n",
        " "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "CNNPONG.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "2155bc18c00ab7f8a4df30c8cbb3ff6880140f5f09c426d3373023701355f144"
    },
    "kernelspec": {
      "display_name": "Python 3.9.10 64-bit (windows store)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
