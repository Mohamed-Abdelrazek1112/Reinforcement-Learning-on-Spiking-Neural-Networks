{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bIsjZSqSgmhz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# USING THE Training data generated from a ANN-DQN and trying to predict what action to take based on the image that is 4 images stacked "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8cP6jwQmgmh1"
      },
      "outputs": [],
      "source": [
        "# General libraries\n",
        "import pickle\n",
        "#For model building\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARVfYf55gmh2"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "from gym import wrappers\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import imageio\n",
        "import numpy as np\n",
        "from skimage.transform import resize\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import torchvision.transforms as transforms\n",
        "import tensorflow.compat.v1 as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fLvG0t9Igmh2"
      },
      "outputs": [],
      "source": [
        "class Atari(object):\n",
        "    \"\"\"Wrapper for the environment provided by gym\"\"\"\n",
        "    def __init__(self, envName, no_op_steps=10, agent_history_length=4):\n",
        "        self.env = gym.make(envName)\n",
        "        self.state = None\n",
        "        self.last_lives = 0\n",
        "        self.no_op_steps = no_op_steps\n",
        "        self.agent_history_length = agent_history_length\n",
        "\n",
        "    def reset(self,evaluation=False):\n",
        "\n",
        "        frame = self.env.reset()\n",
        "        self.last_lives = 0\n",
        "        terminal_life_lost = True # Set to true so that the agent starts \n",
        "                                  # with a 'FIRE' action when evaluating\n",
        "        if evaluation:\n",
        "            for _ in range(random.randint(1, self.no_op_steps)):\n",
        "                frame, _, _, _ = self.env.step(1) # Action 'Fire'\n",
        "        processed_frame = frameprocess(frame)   # (★★★)\n",
        "        self.state = np.repeat(processed_frame, self.agent_history_length, axis=2)\n",
        "        \n",
        "        return terminal_life_lost\n",
        "\n",
        "    def step(self,action):\n",
        "\n",
        "        new_frame, reward, terminal, info = self.env.step(action)  # (5★)\n",
        "            \n",
        "        if info['lives'] < self.last_lives:\n",
        "            terminal_life_lost = True\n",
        "        else:\n",
        "            terminal_life_lost = terminal\n",
        "        self.last_lives = info['lives']\n",
        "        \n",
        "        processed_new_frame = frameprocess(new_frame)   # (6★)\n",
        "        new_state = np.append(self.state[:, :, 1:], processed_new_frame, axis=2) # (6★)   \n",
        "        self.state = new_state\n",
        "        \n",
        "        return processed_new_frame, reward, terminal, terminal_life_lost, new_frame"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZDt-GtVgmh3"
      },
      "source": [
        "# Loads the training data and cleans it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44waaOOdgmh5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "id": "uFSzODTAgmh5",
        "outputId": "aa66ab35-d0b3-4ce0-d7b1-8a4e3012b670"
      },
      "outputs": [],
      "source": [
        "totensor = transforms.ToTensor()\n",
        "\n",
        "with open('pickeledDataPong', 'rb') as f:\n",
        "    trainingData = pickle.load(f)\n",
        "# Reformat training data to predict the next 20 moves\n",
        "trainset,testset = train_test_split(trainingData,test_size=0.3)\n",
        "l = len(trainset)\n",
        "trainimages = []\n",
        "trainlabels = []\n",
        "testimages = []\n",
        "testlabels = []\n",
        "for i in range(l):\n",
        "    if trainset[i][1] not in [0,1,2]:\n",
        "        trainimages.append(totensor(np.array(trainset[i][0])[:,14:66]).cpu().detach().numpy())\n",
        "        trainlabels.append(np.array([k==trainset[i][1] for k in range(6)]))\n",
        "lt = len(testset)\n",
        "for i in range(lt):\n",
        "    if testset[i][1] not in [0,1,2]:\n",
        "        testimages.append(totensor(np.array(testset[i][0])[:,14:66]).cpu().detach().numpy())\n",
        "        testlabels.append(testset[i][1])\n",
        "\n",
        "\n",
        "\n",
        "train_data = TensorDataset(torch.Tensor(np.array(trainimages)), torch.Tensor(np.array(trainlabels)))\n",
        "test_data = TensorDataset(torch.Tensor(np.array(testimages)),torch.Tensor(np.array(testlabels)))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A65FQBfugmh6"
      },
      "outputs": [],
      "source": [
        "def frameprocess(frame,frame_height=84, frame_width=65):\n",
        "\n",
        "    frame_height = frame_height\n",
        "    frame_width = frame_width\n",
        "    processed = tf.image.rgb_to_grayscale(frame)\n",
        "    processed = tf.image.crop_to_bounding_box(processed, 34, 0, 160, 140)\n",
        "    processed = tf.image.resize_images(processed, \n",
        "                                            [frame_height, frame_width], \n",
        "                                            method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "    return processed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_VIV5l7fgmh7"
      },
      "outputs": [],
      "source": [
        "def generate_gif(frames_for_gif):\n",
        "\n",
        "    for idx, frame_idx in enumerate(frames_for_gif): \n",
        "        frames_for_gif[idx] = resize(frame_idx, (420, 320, 3), \n",
        "                                     preserve_range=True, order=0).astype(np.uint8)\n",
        "        \n",
        "    imageio.mimsave(\"ATARI_PONG.gif\", \n",
        "                    frames_for_gif, duration=1/30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BtyyUwKWgmh8"
      },
      "outputs": [],
      "source": [
        "#newTrainingData[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-er9Fvpgmh8"
      },
      "outputs": [],
      "source": [
        "\"\"\"# import required libraries\n",
        "import torch\n",
        "import torchvision.transforms as T\n",
        "from PIL import Image\n",
        "\n",
        "# define a torch tensor\n",
        "print(trainingData[1000][0].shape)\n",
        "tensor = newTrainingData[1000][0]\n",
        "\n",
        "# define a transform to convert a tensor to PIL image\n",
        "transform = T.ToPILImage()\n",
        "nd = transforms.ToTensor()\n",
        "# convert the tensor to PIL image using above transform\n",
        "img = transform(tensor)\n",
        "imgten = nd(newTrainingData[1000][0])\n",
        "print(imgten.shape)\n",
        "# display the PIL image\n",
        "img.show()\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_t2xWC7-gmh9"
      },
      "outputs": [],
      "source": [
        "len(train_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MG4gqvs8gmh9"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dq7gatb_gmh-"
      },
      "outputs": [],
      "source": [
        "batch_size = 1\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,shuffle=True,drop_last=True, num_workers=2)\n",
        "testloader = torch.utils.data.DataLoader(test_data, batch_size=batch_size,shuffle=True,drop_last=True,num_workers=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIVmlzWogmh-"
      },
      "outputs": [],
      "source": [
        "\"\"\" for image,label in trainloader:\n",
        "    # define a torch tensor\n",
        "    print(image[0].shape)\n",
        "    tensor = image[0]\n",
        "\n",
        "    # define a transform to convert a tensor to PIL image\n",
        "    transform = T.ToPILImage()\n",
        "\n",
        "    # convert the tensor to PIL image using above transform\n",
        "    img = transform(tensor)\n",
        "\n",
        "    # display the PIL image\n",
        "    img.show()\n",
        "    break\"\"\"\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8qd-xK89gmh-"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(4, 6,8,4)\n",
        "        self.conv2 = nn.Conv2d(6, 16,4,2)\n",
        "        self.conv3 = nn.Conv2d(16,32,3,1)\n",
        "        self.conv4 = nn.Conv2d(32,64,7,1)\n",
        "        \n",
        "        self.fc1 = nn.Linear(720*batch_size, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fclast = nn.Linear(84, 6*batch_size)\n",
        "        self.softmax = torch.nn.Softmax()\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        \n",
        "        x = torch.flatten(x) # flatten all dimensions\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))    \n",
        "        x =self.fclast(x)    \n",
        "        sig_out = x.view(batch_size, -1)\n",
        "        #print(x)\n",
        "        sig_out = self.softmax(sig_out)\n",
        "        #print(\"sig x\",sig_out)\n",
        "        #print(sig_out)\n",
        "        return sig_out\n",
        "\n",
        "\n",
        "net = Net()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EaRditSrgmh-"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBlGJURJgmh-"
      },
      "outputs": [],
      "source": [
        "# zero the parameter gradients\n",
        "optimizer.zero_grad()\n",
        "# forward + backward + optimize\n",
        "#print(torch.tensor([input]).size())\n",
        "for epoch in range(1):\n",
        "    for image,move in trainloader:\n",
        "        outputs = net(image)\n",
        "        #print(torch.tensor(outputs))\n",
        "        #print(move)\n",
        "        #print(torch.argmax(outputs),torch.tensor(move))\n",
        "        loss = criterion(outputs,torch.tensor(move))\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uV7W1_qugmh_"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "# zero the parameter gradients\n",
        "optimizer.zero_grad()\n",
        "\n",
        "for input,labels in trainloader:\n",
        "    # forward + backward + optimize\n",
        "    #print(torch.tensor([input]).size())\n",
        "    outputs = net(torch.tensor(input))\n",
        "    loss = sum([criterion(output, l) for (l, output) in zip(outputs, labels)])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zEe2iZDogmh_"
      },
      "outputs": [],
      "source": [
        "def check_accuracy(outputs,target):\n",
        "    #print(outputs)\n",
        "    return np.equal(np.array([torch.argmax(outputs)]),np.array(target))\n",
        "\n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eWxpQmcUgmh_"
      },
      "outputs": [],
      "source": [
        "accuracy = []\n",
        "for image,moves in testloader:\n",
        "    outputs = net(image)\n",
        "    #print(check_accuracy(outputs,moves))\n",
        "    accuracy.append(check_accuracy(outputs,moves))\n",
        "    #print(torch.argmax(outputs))\n",
        "\n",
        "print(np.mean(accuracy))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-MdjYQeUgmh_"
      },
      "outputs": [],
      "source": [
        "torch.save(net.state_dict(), \"savedmodel\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-i4VgFgCgmh_"
      },
      "outputs": [],
      "source": [
        "torch.save(net, \"FULLMODEL\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19OUJdw4gmiA"
      },
      "outputs": [],
      "source": [
        "#2up 3 down 4 up 5 down"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lqt2XwoEgmiA"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7BGDjlpgmiA"
      },
      "outputs": [],
      "source": [
        "#tt = transforms.ToTensor()\n",
        "env = Atari(\"PongDeterministic-v4\")\n",
        "env.reset()\n",
        "frames=[]\n",
        "total_reward=0\n",
        "done = False\n",
        "env.reset()\n",
        "i=0\n",
        "\n",
        "for i in range(300):\n",
        "    action = 2\n",
        "    observation, reward, done, info,new_frame = env.step(action)\n",
        "    frames.append(new_frame)\n",
        "    env.env.render()\n",
        "#transform = transforms.ToPILImage()\n",
        "\n",
        "# convert the tensor to PIL image using above transform\n",
        "#img = transform(tt(env.state))\n",
        "\n",
        "# display the PIL image\n",
        "#img.save(\"PIL.png\")\n",
        "    \n",
        "while not done:\n",
        "    i+=1\n",
        "    #s = env.state\n",
        "    action = net(torch.Tensor(np.array([totensor(env.state).cpu().detach().numpy() for i in range(batch_size)])))\n",
        "    #print(\"action\",action)\n",
        "    #env.step(action)\n",
        "    #print(torch.argmax(action))\n",
        "    observation, reward, done, info,new_frame = env.step(int(torch.argmax(action[0])))\n",
        "    #(env.state).show()\n",
        "    \n",
        "    frames.append(new_frame)\n",
        "    env.env.render()\n",
        "\n",
        "print(i)\n",
        "env.reset()\n",
        "generate_gif(frames)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Idea Abandoned As 40% accuracy seems too bad\n",
        "# Was planning on converting to SNN but not worth it as some accuarcy will be lost in the process\n",
        " "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "CNNPONG.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "2155bc18c00ab7f8a4df30c8cbb3ff6880140f5f09c426d3373023701355f144"
    },
    "kernelspec": {
      "display_name": "Python 3.9.10 64-bit (windows store)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
