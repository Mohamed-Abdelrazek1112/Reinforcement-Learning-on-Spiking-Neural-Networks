{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install Needed libraries# python 3.8\n",
    "%pip install gym -qqq\n",
    "%pip install pygame -qqq\n",
    "%pip install gym[accept-rom-license] -qqq\n",
    "%pip install spikingjelly==0.0.0.0.4 -qqq\n",
    "%pip install cupy -qqq\n",
    "%pip install ale-py -qqq\n",
    "%pip install tensorflow -qqq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Makes Heavy Use of the Spiking Jelly Repo Though had to modify some of the libraries code to get it to work with images as in default it was meant to work with states that describe the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from spikingjelly.clock_driven import neuron, functional\n",
    "import os\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = \"Breakout-v0\"\n",
    "import cv2\n",
    "import time\n",
    "import imageio\n",
    "import gym\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from skimage.transform import resize\n",
    "from collections import deque\n",
    "import random\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.compat.v1 as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gif(frames_for_gif,name):\n",
    "    for idx, frame_idx in enumerate(frames_for_gif): \n",
    "        frames_for_gif[idx] = resize(frame_idx, (420, 320, 3), \n",
    "                                     preserve_range=True, order=0).astype(np.uint8)\n",
    "        \n",
    "    imageio.mimsave(name+str(game)+\".gif\", frames_for_gif, duration=1/30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frameprocess(frame,frame_height=84, frame_width=84):\n",
    "    frame_height = frame_height\n",
    "    frame_width = frame_width\n",
    "    processed = tf.image.rgb_to_grayscale(frame)\n",
    "    processed = tf.image.crop_to_bounding_box(processed, 34, 0, 160, 140)\n",
    "    processed = tf.image.resize_images(processed, \n",
    "                                            [frame_height, frame_width], \n",
    "                                            method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Atari(object):\n",
    "    \"\"\"Wrapper for the environment provided by gym\"\"\"\n",
    "    def __init__(self, envName, no_op_steps=10, agent_history_length=4):\n",
    "        self.env = gym.make(envName)\n",
    "        self.unwrapped = self.env.unwrapped\n",
    "        self.state = None\n",
    "        self.totalPixels = 84*84*4\n",
    "        self.last_lives = 0\n",
    "        self.no_op_steps = no_op_steps\n",
    "        self.metadata = self.env.metadata\n",
    "        self.agent_history_length = agent_history_length\n",
    "        self.spec = self.env.spec\n",
    "        self.action_space = self.env.action_space\n",
    "        self.render = self.env.render\n",
    "\n",
    "    def reset(self,evaluation=False):\n",
    "        frame = self.env.reset()\n",
    "        self.last_lives = 0\n",
    "        processed_frame = frameprocess(frame)  \n",
    "        self.state = np.repeat(processed_frame, self.agent_history_length, axis=2)\n",
    "\n",
    "    def step(self,action):\n",
    "        new_frame, reward, done, info = self.env.step(action)\n",
    "        processed_new_frame = frameprocess(new_frame) \n",
    "        new_state = np.append(self.state[:, :, 1:], processed_new_frame, axis=2)    \n",
    "        self.state = new_state\n",
    "        return processed_new_frame, reward, done, new_frame\n",
    "    \n",
    "    def seed(self,seed):\n",
    "        self.env.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',('state', 'action', 'next_state', 'reward'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Make SNN output floating numbers. Firing threshold of a neuron is set to be infinity, which wonâ€™t fire at all, and we adopt the final membrane potential to represent Q function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        #print(\"TRANSITION\",Transition(*args))\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "class NonSpikingLIFNode(neuron.LIFNode):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args,**kwargs)\n",
    "\n",
    "    def forward(self, dv: torch.Tensor):\n",
    "        self.neuronal_charge(dv)\n",
    "        self.neuronal_fire()\n",
    "        self.neuronal_reset()\n",
    "        return self.v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spiking DQN algorithm\n",
    "class DQSN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, T=16):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            neuron.IFNode(),\n",
    "            nn.Linear(hidden_size, output_size),\n",
    "            NonSpikingLIFNode(tau=2.0)\n",
    "        )\n",
    "\n",
    "        self.T = T\n",
    "\n",
    "    def forward(self, x):\n",
    "        for t in range(self.T):\n",
    "            self.fc(x)\n",
    "            \n",
    "        return self.fc[-1].v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(use_cuda, model_dir, log_dir, env_name, hidden_size, num_episodes, seed):\n",
    "    BATCH_SIZE = 128\n",
    "    GAMMA = 0.999\n",
    "    EPS_START = 0.9\n",
    "    EPS_END = 0.05\n",
    "    EPS_DECAY = 200\n",
    "    TARGET_UPDATE = 10\n",
    "\n",
    "    T = 16\n",
    "\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    steps_done = 0\n",
    "\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    #env = gym.make(env_name).unwrapped\n",
    "    #env.seed(seed)\n",
    "    #n_states = env.observation_space.shape[0]\n",
    "    #n_actions = env.action_space.n\n",
    "    \n",
    "    env = Atari(env_name)\n",
    "    n_states = env.totalPixels\n",
    "    n_actions = env.action_space.n\n",
    "    \n",
    "    ##print(\"STATES\",n_states,\"ACTIONS\",n_actions)\n",
    "\n",
    "    policy_net = DQSN(n_states, hidden_size, n_actions, T).to(device)\n",
    "    target_net = DQSN(n_states, hidden_size, n_actions, T).to(device)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    target_net.eval()\n",
    "\n",
    "    optimizer = optim.Adam(policy_net.parameters())\n",
    "    memory = ReplayMemory(10000)\n",
    "\n",
    "    def select_action(state, steps_done):\n",
    "        sample = random.random()\n",
    "        eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "                        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "        if sample > eps_threshold:\n",
    "            #Breakout\n",
    "            with torch.no_grad():\n",
    "                ac = torch.tensor([[torch.argmax(policy_net(state))]])\n",
    "                #print(\"AC\",ac)\n",
    "                functional.reset_net(policy_net)\n",
    "                return ac\n",
    "        else:\n",
    "            #print(\"RANDOM\",torch.tensor([[random.randrange(env.action_space.n)]], device=device, dtype=torch.long))\n",
    "            return torch.tensor([[random.randrange(env.action_space.n)]], device=device, dtype=torch.long)\n",
    "\n",
    "    def optimize_model():\n",
    "        if len(memory) < BATCH_SIZE:\n",
    "            return\n",
    "\n",
    "        transitions = memory.sample(BATCH_SIZE)\n",
    "\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,batch.next_state)), device=device, dtype=torch.bool)\n",
    "        \n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "                                                    \n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "        state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "        next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "        functional.reset_net(target_net)\n",
    "        expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "        loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        for param in policy_net.parameters():\n",
    "            if param.grad is not None:\n",
    "                param.grad.data.clamp_(-1, 1)\n",
    "        optimizer.step()\n",
    "        functional.reset_net(policy_net)\n",
    "\n",
    "    max_reward = 0\n",
    "    max_pt_path = os.path.join(model_dir, f'saved_net_{hidden_size}_max.pt')\n",
    "    pt_path = os.path.join(model_dir, f'saved_net_{hidden_size}.pt')\n",
    "\n",
    "    for i_episode in range(num_episodes):\n",
    "        # Initialize the environment and state\n",
    "        env.reset()\n",
    "        state = torch.zeros([1, n_states], dtype=torch.float, device=device)\n",
    "\n",
    "        total_reward = 0\n",
    "\n",
    "        for t in count():\n",
    "            action = select_action(state, steps_done)\n",
    "            steps_done += 1\n",
    "            #print(\"ACTION\",action)\n",
    "            next_state, reward, done, _ = env.step(action.item())\n",
    "            next_state = env.state\n",
    "\n",
    "            #BREAKOUT\n",
    "            next_state = np.reshape(next_state,(1,84*84*4))\n",
    "            \n",
    "            #print(\"STATE SHAPE\",next_state.shape)\n",
    "            total_reward += reward\n",
    "            next_state = torch.from_numpy(next_state).float().to(device).unsqueeze(0)\n",
    "            \n",
    "            \n",
    "            reward = torch.tensor([reward], device=device)\n",
    "\n",
    "            if done:\n",
    "                next_state = None\n",
    "\n",
    "            if(state!=None and len(state.shape)==3):\n",
    "                state = torch.reshape(state, (1,84*84*4))\n",
    "            if(next_state!=None and len(next_state.shape)==3):\n",
    "                next_state = torch.reshape(next_state, (1,84*84*4))\n",
    "\n",
    "            #print(len(action.shape),len(reward.shape))\n",
    "            memory.push(state, action, next_state, reward)\n",
    "\n",
    "            state = next_state\n",
    "            if done and total_reward > max_reward:\n",
    "                max_reward = total_reward\n",
    "                torch.save(policy_net.state_dict(), max_pt_path)\n",
    "                print(f'max_reward={max_reward}, save models')\n",
    "\n",
    "            optimize_model()\n",
    "\n",
    "            if done:\n",
    "                print(f'Episode: {i_episode}, Reward: {total_reward}')\n",
    "                writer.add_scalar('Spiking-DQN-state-' + env_name + '/Reward', total_reward, i_episode)\n",
    "                break\n",
    "\n",
    "        if i_episode % TARGET_UPDATE == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "    print('complete')\n",
    "    torch.save(policy_net.state_dict(), pt_path)\n",
    "    print('state_dict path is', pt_path)\n",
    "\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(use_cuda, pt_path, env_name, hidden_size, played_frames=60, save_fig_num=0, fig_dir=None, figsize=(12, 6), firing_rates_plot_type='bar', heatmap_shape=None):    \n",
    "    T = 16\n",
    "    FRAMESTORE = []\n",
    "    SCREENSTORE = []\n",
    "    plt.rcParams['figure.figsize'] = figsize\n",
    "    plt.ion()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    env = Atari(env_name)\n",
    "    n_states = env.totalPixels\n",
    "    n_actions = env.action_space.n\n",
    "\n",
    "    policy_net = DQSN(n_states, hidden_size, n_actions, T).to(device)\n",
    "    policy_net.load_state_dict(torch.load(pt_path, map_location=device))\n",
    "\n",
    "    env.reset()\n",
    "    state = torch.zeros([1, n_states], dtype=torch.float, device=device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        #functional.set_monitor(policy_net, True)\n",
    "        delta_lim = 0\n",
    "        over_score = 1e9\n",
    "\n",
    "        for i in count():\n",
    "            LIF_v = policy_net(state)\n",
    "            action = torch.tensor([[torch.argmax(LIF_v)]])\n",
    "\n",
    "            if firing_rates_plot_type == 'bar':\n",
    "                plt.subplot2grid((2, 9), (1, 0), colspan=3)\n",
    "            elif firing_rates_plot_type == 'heatmap':\n",
    "                plt.subplot2grid((2, 3), (1, 0))\n",
    "\n",
    "            plt.xticks(np.arange(4), (meaning for meaning in env.unwrapped.get_action_meanings()))\n",
    "            plt.ylabel('Voltage')\n",
    "            plt.title('Voltage of LIF neurons at last time step')\n",
    "            delta_lim = (LIF_v.max() - LIF_v.min()) * 0.5\n",
    "            plt.ylim(LIF_v.min() - delta_lim, LIF_v.max() + delta_lim)\n",
    "            plt.yticks([])\n",
    "            plt.text(0, LIF_v[0][0], str(round(LIF_v[0][0].item(), 2)), ha='center')\n",
    "            plt.text(1, LIF_v[0][1], str(round(LIF_v[0][1].item(), 2)), ha='center')\n",
    "\n",
    "            plt.bar(np.arange(4), LIF_v.squeeze(), color=['r', 'gray'] if action == 0 else ['gray', 'r'], width=0.5)\n",
    "            \n",
    "            if LIF_v.min() - delta_lim < 0:\n",
    "                plt.axhline(0, color='black', linewidth=0.1)\n",
    "\n",
    "            policy_net.fc[1].set_monitor()\n",
    "            IF_spikes = np.asarray(policy_net.fc[1].monitor['s'])\n",
    "            firing_rates = IF_spikes.mean(axis=0).squeeze()\n",
    "            \n",
    "            if firing_rates_plot_type == 'bar':\n",
    "                plt.subplot2grid((2, 9), (0, 4), rowspan=2, colspan=5)\n",
    "            elif firing_rates_plot_type == 'heatmap':\n",
    "                plt.subplot2grid((2, 3), (0, 1), rowspan=2, colspan=2)\n",
    "            \n",
    "            plt.title('Firing rates of IF neurons')\n",
    "\n",
    "            if firing_rates_plot_type == 'bar':\n",
    "                plt.xlabel('Neuron index')\n",
    "                plt.ylabel('Firing rate')\n",
    "                plt.xlim(0, firing_rates.size)\n",
    "                plt.ylim(0, 1.01)\n",
    "                plt.bar(np.arange(firing_rates.size), firing_rates, width=0.5)\n",
    "\n",
    "            elif firing_rates_plot_type == 'heatmap':\n",
    "                heatmap = plt.imshow(firing_rates.reshape(heatmap_shape), vmin=0, vmax=1, cmap='ocean')\n",
    "                plt.gca().invert_yaxis()\n",
    "                cbar = heatmap.figure.colorbar(heatmap)\n",
    "                cbar.ax.set_ylabel('Magnitude', rotation=90, va='top')\n",
    "            \n",
    "\n",
    "            state, reward, done, obs = env.step(action)\n",
    "            FRAMESTORE.append(obs)\n",
    "\n",
    "            subtitle = \"\"\n",
    "            if done:\n",
    "                over_score = min(over_score, i)\n",
    "                subtitle = f'Game over, Score={over_score}'\n",
    "            plt.suptitle(subtitle)\n",
    "            \n",
    "            state = np.reshape(env.state,(84*84*4))\n",
    "            state = torch.from_numpy(state).float().to(device).unsqueeze(0)\n",
    "            screen = env.render(mode='rgb_array').copy()\n",
    "            screen[200, :, :] = 0\n",
    "            \n",
    "            if firing_rates_plot_type == 'bar':\n",
    "                plt.subplot2grid((2, 9), (0, 0), colspan=3)\n",
    "            elif firing_rates_plot_type == 'heatmap':\n",
    "                plt.subplot2grid((2, 3), (0, 0))\n",
    "            \n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "            plt.title('Game screen')\n",
    "            plt.imshow(screen, interpolation='bicubic')\n",
    "            plt.pause(0.001)\n",
    "            \n",
    "            if i < save_fig_num:\n",
    "                plt.savefig(os.path.join(fig_dir, f'{i}.png'))\n",
    "            \n",
    "            if done and i >= played_frames:\n",
    "                env.close()\n",
    "                plt.close()\n",
    "                break\n",
    "    generate_gif(FRAMESTORE,\"GAMEFOOTAGE\")\n",
    "    generate_gif(SCREENSTORE,\"SCREENFOOTAGE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(use_cuda=False, model_dir='./', log_dir='./log', env_name='Breakout-v0', hidden_size=256, num_episodes=100000, seed=random.randint(1,10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play(use_cuda=False, pt_path='./saved_net_256_max.pt', env_name='Breakout-v0', hidden_size=256, played_frames=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credits to\n",
    "## https://github.com/fangwei123456/spikingjelly\n",
    "## https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f47b34d28df447fa8fc4c7984535c7007664f477d5d31b3931e5c7b6b9caa621"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
