{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import gym\n",
    "from gym import wrappers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "ti = time.time()\n",
    "import imageio\n",
    "import numpy as np\n",
    "from skimage.transform import resize\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "import tensorflow.compat.v1 as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gif(frames_for_gif):\n",
    "    for idx, frame_idx in enumerate(frames_for_gif): \n",
    "        frames_for_gif[idx] = resize(frame_idx, (420, 320, 3), \n",
    "                                     preserve_range=True, order=0).astype(np.uint8)\n",
    "        \n",
    "    imageio.mimsave(\"ATARI_PONG.gif\", \n",
    "                    frames_for_gif, duration=1/30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frameprocess(frame,frame_height=84, frame_width=84):\n",
    "    frame_height = frame_height\n",
    "    frame_width = frame_width\n",
    "    processed = tf.image.rgb_to_grayscale(frame)\n",
    "    processed = tf.image.crop_to_bounding_box(processed, 34, 0, 160, 160)\n",
    "    processed = tf.image.resize_images(processed, \n",
    "                                            [frame_height, frame_width], \n",
    "                                            method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import Tuple, Dict, Any\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from bindsnet.datasets.preprocess import subsample, gray_scale, binary_image, crop\n",
    "from bindsnet.encoding import Encoder, NullEncoder\n",
    "\n",
    "\n",
    "class Environment(ABC):\n",
    "    # language=rst\n",
    "    \"\"\"\n",
    "    Abstract environment class.\n",
    "    \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def step(self, a: int) -> Tuple[Any, ...]:\n",
    "        # language=rst\n",
    "        \"\"\"\n",
    "        Abstract method head for ``step()``.\n",
    "\n",
    "        :param a: Integer action to take in environment.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def reset(self) -> None:\n",
    "        # language=rst\n",
    "        \"\"\"\n",
    "        Abstract method header for ``reset()``.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def render(self) -> None:\n",
    "        # language=rst\n",
    "        \"\"\"\n",
    "        Abstract method header for ``render()``.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def close(self) -> None:\n",
    "        # language=rst\n",
    "        \"\"\"\n",
    "        Abstract method header for ``close()``.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def preprocess(self) -> None:\n",
    "        # language=rst\n",
    "        \"\"\"\n",
    "        Abstract method header for ``preprocess()``.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class GymEnvironment(Environment):\n",
    "    # language=rst\n",
    "    \"\"\"\n",
    "    A wrapper around the OpenAI ``gym`` environments.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name: str, encoder: Encoder = NullEncoder(), **kwargs) -> None:\n",
    "        # language=rst\n",
    "        \"\"\"\n",
    "        Initializes the environment wrapper. This class makes the\n",
    "        assumption that the OpenAI ``gym`` environment will provide an image\n",
    "        of format HxW or CxHxW as an observation (we will add the C\n",
    "        dimension to HxW tensors) or a 1D observation in which case no\n",
    "        dimensions will be added.\n",
    "\n",
    "        :param name: The name of an OpenAI ``gym`` environment.\n",
    "        :param encoder: Function to encode observations into spike trains.\n",
    "\n",
    "        Keyword arguments:\n",
    "\n",
    "        :param float max_prob: Maximum spiking probability.\n",
    "        :param bool clip_rewards: Whether or not to use ``np.sign`` of rewards.\n",
    "\n",
    "        :param int history: Number of observations to keep track of.\n",
    "        :param int delta: Step size to save observations in history.\n",
    "        :param bool add_channel_dim: Allows for the adding of the channel dimension in\n",
    "            2D inputs.\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "        self.frames = []\n",
    "        self.env = gym.make(name)\n",
    "        self.action_space = self.env.action_space\n",
    "        self.no_op_steps = 10\n",
    "        self.agent_history_length = 4\n",
    "        self.state = None\n",
    "        self.encoder = encoder\n",
    "        # Keyword arguments.\n",
    "        self.max_prob = kwargs.get(\"max_prob\", 1.0)\n",
    "        self.clip_rewards = kwargs.get(\"clip_rewards\", True)\n",
    "\n",
    "        self.history_length = kwargs.get(\"history_length\", None)\n",
    "        self.delta = kwargs.get(\"delta\", 1)\n",
    "        self.add_channel_dim = kwargs.get(\"add_channel_dim\", True)\n",
    "\n",
    "        if self.history_length is not None and self.delta is not None:\n",
    "            self.history = {\n",
    "                i: torch.Tensor()\n",
    "                for i in range(1, self.history_length * self.delta + 1, self.delta)\n",
    "            }\n",
    "        else:\n",
    "            self.history = {}\n",
    "\n",
    "        self.episode_step_count = 0\n",
    "        self.history_index = 1\n",
    "\n",
    "        self.obs = None\n",
    "        self.reward = None\n",
    "\n",
    "        assert (\n",
    "            0.0 < self.max_prob <= 1.0\n",
    "        ), \"Maximum spiking probability must be in (0, 1].\"\n",
    "        \n",
    "    def get_cart_location(self,screen_width):\n",
    "        world_width = self.env.x_threshold * 2\n",
    "        scale = screen_width / world_width\n",
    "        return int(self.env.state[0] * scale + screen_width / 2.0)  # MIDDLE OF CART\n",
    "\n",
    "    def get_screen(self):\n",
    "        # Returned screen requested by gym is 400x600x3, but is sometimes larger\n",
    "        # such as 800x1200x3. Transpose it into torch order (CHW).\n",
    "        screen = self.env.render(mode='rgb_array').transpose((2, 0, 1))\n",
    "        # Cart is in the lower half, so strip off the top and bottom of the screen\n",
    "        _, screen_height, screen_width = screen.shape\n",
    "        screen = screen[:, int(screen_height*0.4):int(screen_height * 0.8)]\n",
    "        view_width = int(screen_width * 0.6)\n",
    "        cart_location = self.get_cart_location(screen_width)\n",
    "        if cart_location < view_width // 2:\n",
    "            slice_range = slice(view_width)\n",
    "        elif cart_location > (screen_width - view_width // 2):\n",
    "            slice_range = slice(-view_width, None)\n",
    "        else:\n",
    "            slice_range = slice(cart_location - view_width // 2,\n",
    "                                cart_location + view_width // 2)\n",
    "        # Strip off the edges, so that we have a square image centered on a cart\n",
    "        screen = screen[:, :, slice_range]\n",
    "        # Convert to float, rescale, convert to torch tensor\n",
    "        # (this doesn't require a copy)\n",
    "        screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "        screen = torch.from_numpy(screen)\n",
    "        # Resize, and add a batch dimension (BCHW)\n",
    "        return resize(screen,(80,80))#.unsqueeze(0)\n",
    "\n",
    "        \n",
    "        \n",
    "    def generate_gif(self,name):\n",
    "        frames_for_gif = self.frames\n",
    "        for idx, frame_idx in enumerate(frames_for_gif): \n",
    "            frames_for_gif[idx] = resize(frame_idx, (420, 320, 3), preserve_range=True, order=0).astype(np.uint8)\n",
    "            \n",
    "        imageio.mimsave(str(name)+\"ATARI_PONG.gif\", frames_for_gif, duration=1/30)\n",
    "\n",
    "    def step(self, a: int) -> Tuple[torch.Tensor, float, bool, Dict[Any, Any]]:\n",
    "        # language=rst\n",
    "        \"\"\"\n",
    "        Wrapper around the OpenAI ``gym`` environment ``step()`` function.\n",
    "\n",
    "        :param a: Action to take in the environment.\n",
    "        :return: Observation, reward, done flag, and information dictionary.\n",
    "        \"\"\"\n",
    "        # Call gym's environment step function.\n",
    "        self.obs, self.reward, self.done, info = self.env.step(a)\n",
    "        self.frames.append(self.obs)\n",
    "        #processed_new_frame = frameprocess(self.obs)   # (6★)\n",
    "        #new_state = np.append(self.state[:, :, 1:], processed_new_frame, axis=2) # (6★)   \n",
    "        #self.state = new_state\n",
    "\n",
    "        if self.clip_rewards:\n",
    "            self.reward = np.sign(self.reward)\n",
    "\n",
    "        self.preprocess()\n",
    "\n",
    "        # Add the raw observation from the gym environment into the info\n",
    "        # for debugging and display.\n",
    "        info[\"gym_obs\"] = self.obs\n",
    "\n",
    "        # Store frame of history and encode the inputs.\n",
    "        if len(self.history) > 0:\n",
    "            self.update_history()\n",
    "            self.update_index()\n",
    "            # Add the delta observation into the info for debugging and display.\n",
    "            info[\"delta_obs\"] = self.obs\n",
    "\n",
    "        # The new standard for images is BxTxCxHxW.\n",
    "        # The gym environment doesn't follow exactly the same protocol.\n",
    "        #\n",
    "        # 1D observations will be left as is before the encoder and will become BxTxL.\n",
    "        # 2D observations are assumed to be mono images will become BxTx1xHxW\n",
    "        # 3D observations will become BxTxCxHxW\n",
    "        print(self.obs.dim)\n",
    "        if self.obs.dim() == 2 and self.add_channel_dim:\n",
    "            # We want CxHxW, it is currently HxW.\n",
    "            self.obs = self.obs.unsqueeze(0)\n",
    "\n",
    "        # The encoder will add time - now Tx...\n",
    "        if self.encoder is not None:\n",
    "            self.obs = self.encoder(self.obs)\n",
    "\n",
    "        # Add the batch - now BxTx...\n",
    "        self.obs = self.obs.unsqueeze(0)\n",
    "\n",
    "        self.episode_step_count += 1\n",
    "        \n",
    "        \n",
    "        # Return converted observations and other information.\n",
    "        return self.obs, self.reward, self.done, info\n",
    "\n",
    "    def reset(self) -> torch.Tensor:\n",
    "        # language=rst\n",
    "        \"\"\"\n",
    "        Wrapper around the OpenAI ``gym`` environment ``reset()`` function.\n",
    "\n",
    "        :return: Observation from the environment.\n",
    "        \"\"\"\n",
    "        # Call gym's environment reset function.\n",
    "        self.obs = self.env.reset()\n",
    "        self.preprocess()\n",
    "\n",
    "        self.history = {i: torch.Tensor() for i in self.history}\n",
    "\n",
    "        self.episode_step_count = 0\n",
    "        \n",
    "        \n",
    "        #for _ in range(random.randint(1, self.no_op_steps)):\n",
    "        #    frame, _, _, _ = self.env.step(1) # Action 'Fire'\n",
    "        #processed_frame = frameprocess(frame)   # (★★★)\n",
    "        #self.state = np.repeat(processed_frame, self.agent_history_length, axis=2)\n",
    "        \n",
    "\n",
    "        return self.obs\n",
    "\n",
    "    def render(self) -> None:\n",
    "        # language=rst\n",
    "        \"\"\"\n",
    "        Wrapper around the OpenAI ``gym`` environment ``render()`` function.\n",
    "        \"\"\"\n",
    "        pass\n",
    "        #self.env.render()\n",
    "\n",
    "    def close(self) -> None:\n",
    "        # language=rst\n",
    "        \"\"\"\n",
    "        Wrapper around the OpenAI ``gym`` environment ``close()`` function.\n",
    "        \"\"\"\n",
    "        self.env.close()\n",
    "\n",
    "    def preprocess(self) -> None:\n",
    "        # language=rst\n",
    "        \"\"\"\n",
    "        Pre-processing step for an observation from a ``gym`` environment.\n",
    "        \"\"\"\n",
    "        if self.name == \"SpaceInvaders-v0\":\n",
    "            self.obs = subsample(gray_scale(self.obs), 84, 110)\n",
    "            self.obs = self.obs[26:104, :]\n",
    "            self.obs = binary_image(self.obs)\n",
    "        elif self.name == \"BreakoutDeterministic-v4\":\n",
    "            self.obs = subsample(gray_scale(crop(self.obs, 34, 194, 0, 160)), 80, 80)\n",
    "            self.obs = binary_image(self.obs)\n",
    "        else:  # Default pre-processing step.\n",
    "            self.obs = self.get_screen()\n",
    "            self.obs = subsample(crop(self.obs, 34, 194, 0, 160), 80, 80)\n",
    "            self.obs = binary_image(self.obs)\n",
    "\n",
    "        self.obs = torch.from_numpy(self.obs).float()\n",
    "\n",
    "    def update_history(self) -> None:\n",
    "        # language=rst\n",
    "        \"\"\"\n",
    "        Updates the observations inside history by performing subtraction from most\n",
    "        recent observation and the sum of previous observations. If there are not enough\n",
    "        observations to take a difference from, simply store the observation without any\n",
    "        differencing.\n",
    "        \"\"\"\n",
    "        # Recording initial observations.\n",
    "        if self.episode_step_count < len(self.history) * self.delta:\n",
    "            # Store observation based on delta value.\n",
    "            if self.episode_step_count % self.delta == 0:\n",
    "                self.history[self.history_index] = self.obs\n",
    "        else:\n",
    "            # Take difference between stored frames and current frame.\n",
    "            temp = torch.clamp(self.obs - sum(self.history.values()), 0, 1)\n",
    "\n",
    "            # Store observation based on delta value.\n",
    "            if self.episode_step_count % self.delta == 0:\n",
    "                self.history[self.history_index] = self.obs\n",
    "\n",
    "            assert (\n",
    "                len(self.history) == self.history_length\n",
    "            ), \"History size is out of bounds\"\n",
    "            self.obs = temp\n",
    "\n",
    "    def update_index(self) -> None:\n",
    "        # language=rst\n",
    "        \"\"\"\n",
    "        Updates the index to keep track of history. For example: ``history = 4``,\n",
    "        ``delta = 3`` will produce ``self.history = {1, 4, 7, 10}`` and\n",
    "        ``self.history_index`` will be updated according to ``self.delta`` and will wrap\n",
    "        around the history dictionary.\n",
    "        \"\"\"\n",
    "        if self.episode_step_count % self.delta == 0:\n",
    "            if self.history_index != max(self.history.keys()):\n",
    "                self.history_index += self.delta\n",
    "            else:\n",
    "                # Wrap around the history.\n",
    "                self.history_index = (self.history_index % max(self.history.keys())) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    from abc import ABC, abstractmethod\n",
    "    from typing import Tuple, Dict, Any\n",
    "    import torchvision.transforms as transforms\n",
    "    import gym\n",
    "    import numpy as np\n",
    "    import torch\n",
    "\n",
    "    from bindsnet.datasets.preprocess import subsample, gray_scale, binary_image, crop\n",
    "    from bindsnet.encoding import Encoder, NullEncoder\n",
    "\n",
    "\n",
    "    class Environment(ABC):\n",
    "        # language=rst\n",
    "        \"\"\"\n",
    "        Abstract environment class.\n",
    "        \"\"\"\n",
    "\n",
    "        @abstractmethod\n",
    "        def step(self, a: int) -> Tuple[Any, ...]:\n",
    "            # language=rst\n",
    "            \"\"\"\n",
    "            Abstract method head for ``step()``.\n",
    "\n",
    "            :param a: Integer action to take in environment.\n",
    "            \"\"\"\n",
    "            pass\n",
    "\n",
    "        @abstractmethod\n",
    "        def reset(self) -> None:\n",
    "            # language=rst\n",
    "            \"\"\"\n",
    "            Abstract method header for ``reset()``.\n",
    "            \"\"\"\n",
    "            pass\n",
    "\n",
    "        @abstractmethod\n",
    "        def render(self) -> None:\n",
    "            # language=rst\n",
    "            \"\"\"\n",
    "            Abstract method header for ``render()``.\n",
    "            \"\"\"\n",
    "            pass\n",
    "\n",
    "        @abstractmethod\n",
    "        def close(self) -> None:\n",
    "            # language=rst\n",
    "            \"\"\"\n",
    "            Abstract method header for ``close()``.\n",
    "            \"\"\"\n",
    "            pass\n",
    "\n",
    "        @abstractmethod\n",
    "        def preprocess(self) -> None:\n",
    "            # language=rst\n",
    "            \"\"\"\n",
    "            Abstract method header for ``preprocess()``.\n",
    "            \"\"\"\n",
    "            pass\n",
    "\n",
    "\n",
    "    class GymEnvironment(Environment):\n",
    "        # language=rst\n",
    "        \"\"\"\n",
    "        A wrapper around the OpenAI ``gym`` environments.\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, name: str, encoder: Encoder = NullEncoder(), **kwargs) -> None:\n",
    "            # language=rst\n",
    "            \"\"\"\n",
    "            Initializes the environment wrapper. This class makes the\n",
    "            assumption that the OpenAI ``gym`` environment will provide an image\n",
    "            of format HxW or CxHxW as an observation (we will add the C\n",
    "            dimension to HxW tensors) or a 1D observation in which case no\n",
    "            dimensions will be added.\n",
    "\n",
    "            :param name: The name of an OpenAI ``gym`` environment.\n",
    "            :param encoder: Function to encode observations into spike trains.\n",
    "\n",
    "            Keyword arguments:\n",
    "\n",
    "            :param float max_prob: Maximum spiking probability.\n",
    "            :param bool clip_rewards: Whether or not to use ``np.sign`` of rewards.\n",
    "\n",
    "            :param int history: Number of observations to keep track of.\n",
    "            :param int delta: Step size to save observations in history.\n",
    "            :param bool add_channel_dim: Allows for the adding of the channel dimension in\n",
    "                2D inputs.\n",
    "            \"\"\"\n",
    "            self.name = name\n",
    "            self.frames = []\n",
    "            self.env = gym.make(name)\n",
    "            self.action_space = self.env.action_space\n",
    "            self.no_op_steps = 10\n",
    "            self.agent_history_length = 4\n",
    "            self.state = np.zeros((84,84,4))\n",
    "            self.encoder = encoder\n",
    "\n",
    "            # Keyword arguments.\n",
    "            self.max_prob = kwargs.get(\"max_prob\", 1.0)\n",
    "            self.clip_rewards = kwargs.get(\"clip_rewards\", True)\n",
    "\n",
    "            self.history_length = kwargs.get(\"history_length\", None)\n",
    "            self.delta = kwargs.get(\"delta\", 1)\n",
    "            self.add_channel_dim = kwargs.get(\"add_channel_dim\", True)\n",
    "\n",
    "            if self.history_length is not None and self.delta is not None:\n",
    "                self.history = {\n",
    "                    i: torch.Tensor()\n",
    "                    for i in range(1, self.history_length * self.delta + 1, self.delta)\n",
    "                }\n",
    "            else:\n",
    "                self.history = {}\n",
    "\n",
    "            self.episode_step_count = 0\n",
    "            self.history_index = 1\n",
    "\n",
    "            self.obs = None\n",
    "            self.reward = None\n",
    "\n",
    "            assert (\n",
    "                0.0 < self.max_prob <= 1.0\n",
    "            ), \"Maximum spiking probability must be in (0, 1].\"\n",
    "            \n",
    "        def generate_gif(self,name):\n",
    "            frames_for_gif = self.frames\n",
    "            for idx, frame_idx in enumerate(frames_for_gif): \n",
    "                frames_for_gif[idx] = resize(frame_idx, (420, 320, 3), preserve_range=True, order=0).astype(np.uint8)\n",
    "                \n",
    "            imageio.mimsave(str(name)+\"ATARI_PONG.gif\", frames_for_gif, duration=1/30)\n",
    "\n",
    "        def step(self, a: int) -> Tuple[torch.Tensor, float, bool, Dict[Any, Any]]:\n",
    "            # language=rst\n",
    "            \"\"\"\n",
    "            Wrapper around the OpenAI ``gym`` environment ``step()`` function.\n",
    "\n",
    "            :param a: Action to take in the environment.\n",
    "            :return: Observation, reward, done flag, and information dictionary.\n",
    "            \"\"\"\n",
    "            # Call gym's environment step function.\n",
    "            self.obs, self.reward, self.done, info = self.env.step(a)\n",
    "            self.frames.append(self.obs)\n",
    "            processed_new_frame = frameprocess(self.obs)   # (6★)\n",
    "            new_state = np.append(self.state[:, :, 1:], processed_new_frame, axis=2) # (6★)   \n",
    "            self.state = new_state\n",
    "\n",
    "            if self.clip_rewards:\n",
    "                self.reward = np.sign(self.reward)\n",
    "\n",
    "            self.preprocess()\n",
    "\n",
    "            # Add the raw observation from the gym environment into the info\n",
    "            # for debugging and display.\n",
    "            info[\"gym_obs\"] = self.obs\n",
    "\n",
    "            # Store frame of history and encode the inputs.\n",
    "            if len(self.history) > 0:\n",
    "                self.update_history()\n",
    "                self.update_index()\n",
    "                # Add the delta observation into the info for debugging and display.\n",
    "                info[\"delta_obs\"] = self.obs\n",
    "\n",
    "            # The new standard for images is BxTxCxHxW.\n",
    "            # The gym environment doesn't follow exactly the same protocol.\n",
    "            #\n",
    "            # 1D observations will be left as is before the encoder and will become BxTxL.\n",
    "            # 2D observations are assumed to be mono images will become BxTx1xHxW\n",
    "            # 3D observations will become BxTxCxHxW\n",
    "            if self.obs.dim() == 2 and self.add_channel_dim:\n",
    "                # We want CxHxW, it is currently HxW.\n",
    "                self.obs = self.obs.unsqueeze(0)\n",
    "\n",
    "            # The encoder will add time - now Tx...\n",
    "            if self.encoder is not None:\n",
    "                self.obs = self.encoder(self.obs)\n",
    "\n",
    "            # Add the batch - now BxTx...\n",
    "            self.obs = self.obs.unsqueeze(0)\n",
    "\n",
    "            self.episode_step_count += 1\n",
    "            \n",
    "            \n",
    "            # Return converted observations and other information.\n",
    "            return self.obs, self.reward, self.done, info\n",
    "\n",
    "        def reset(self) -> torch.Tensor:\n",
    "            self.frames = []\n",
    "            # language=rst\n",
    "            \"\"\"\n",
    "            Wrapper around the OpenAI ``gym`` environment ``reset()`` function.\n",
    "\n",
    "            :return: Observation from the environment.\n",
    "            \"\"\"\n",
    "            # Call gym's environment reset function.\n",
    "            self.obs = self.env.reset()\n",
    "            self.preprocess()\n",
    "\n",
    "            self.history = {i: torch.Tensor() for i in self.history}\n",
    "\n",
    "            self.episode_step_count = 0\n",
    "            \n",
    "            \n",
    "            for _ in range(random.randint(1, self.no_op_steps)):\n",
    "                frame, _, _, _ = self.env.step(1) # Action 'Fire'\n",
    "            processed_frame = frameprocess(frame)   # (★★★)\n",
    "            self.state = np.repeat(processed_frame, self.agent_history_length, axis=2)\n",
    "            \n",
    "\n",
    "            return self.obs\n",
    "\n",
    "        def render(self) -> None:\n",
    "            # language=rst\n",
    "            \"\"\"\n",
    "            Wrapper around the OpenAI ``gym`` environment ``render()`` function.\n",
    "            \"\"\"\n",
    "            \n",
    "            #self.env.render()\n",
    "\n",
    "        def close(self) -> None:\n",
    "            # language=rst\n",
    "            \"\"\"\n",
    "            Wrapper around the OpenAI ``gym`` environment ``close()`` function.\n",
    "            \"\"\"\n",
    "            self.env.close()\n",
    "\n",
    "        def preprocess(self) -> None:\n",
    "            # language=rst\n",
    "            \"\"\"\n",
    "            Pre-processing step for an observation from a ``gym`` environment.\n",
    "            \"\"\"\n",
    "            tt = transforms.ToTensor()\n",
    "            if self.name == \"SpaceInvaders-v0\":\n",
    "                #self.obs = self.state\n",
    "                transform = transforms.ToPILImage()\n",
    "                self.obs = tt(self.state)\n",
    "                print(self.obs.dim)\n",
    "            elif self.name == \"BreakoutDeterministic-v4\":\n",
    "                transform = transforms.ToPILImage()\n",
    "                self.obs = tt(self.state)\n",
    "            else:  # Default pre-processing step.\n",
    "                self.obs = tt(self.state)\n",
    "\n",
    "        def update_history(self) -> None:\n",
    "            # language=rst\n",
    "            \"\"\"\n",
    "            Updates the observations inside history by performing subtraction from most\n",
    "            recent observation and the sum of previous observations. If there are not enough\n",
    "            observations to take a difference from, simply store the observation without any\n",
    "            differencing.\n",
    "            \"\"\"\n",
    "            # Recording initial observations.\n",
    "            if self.episode_step_count < len(self.history) * self.delta:\n",
    "                # Store observation based on delta value.\n",
    "                if self.episode_step_count % self.delta == 0:\n",
    "                    self.history[self.history_index] = self.obs\n",
    "            else:\n",
    "                # Take difference between stored frames and current frame.\n",
    "                temp = torch.clamp(self.obs - sum(self.history.values()), 0, 1)\n",
    "\n",
    "                # Store observation based on delta value.\n",
    "                if self.episode_step_count % self.delta == 0:\n",
    "                    self.history[self.history_index] = self.obs\n",
    "\n",
    "                assert (\n",
    "                    len(self.history) == self.history_length\n",
    "                ), \"History size is out of bounds\"\n",
    "                self.obs = temp\n",
    "\n",
    "        def update_index(self) -> None:\n",
    "            # language=rst\n",
    "            \"\"\"\n",
    "            Updates the index to keep track of history. For example: ``history = 4``,\n",
    "            ``delta = 3`` will produce ``self.history = {1, 4, 7, 10}`` and\n",
    "            ``self.history_index`` will be updated according to ``self.delta`` and will wrap\n",
    "            around the history dictionary.\n",
    "            \"\"\"\n",
    "            if self.episode_step_count % self.delta == 0:\n",
    "                if self.history_index != max(self.history.keys()):\n",
    "                    self.history_index += self.delta\n",
    "                else:\n",
    "                    # Wrap around the history.\n",
    "                    self.history_index = (self.history_index % max(self.history.keys())) + 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bindsnet.encoding import bernoulli,poisson\n",
    "#from bindsnet.environment import GymEnvironment\n",
    "from bindsnet.learning import MSTDP\n",
    "from bindsnet.network import Network\n",
    "from bindsnet.network.nodes import Input, LIFNodes\n",
    "from bindsnet.network.topology import Connection\n",
    "from bindsnet.pipeline import EnvironmentPipeline\n",
    "from bindsnet.pipeline.action import select_softmax\n",
    "from collections import Counter\n",
    "# Build network.\n",
    "network = Network(dt=1.0)\n",
    "\n",
    "# Layers of neurons.\n",
    "inpt = Input(n=80 * 80, shape=[1, 1, 1, 80, 80], traces=True)\n",
    "middle = LIFNodes(n=100, traces=True)\n",
    "out = LIFNodes(n=2, refrac=0, traces=True)\n",
    "\n",
    "# Connections between layers.\n",
    "inpt_middle = Connection(source=inpt, target=middle, wmin=-1, wmax=1e-1)\n",
    "middle_out = Connection(\n",
    "    source=middle,\n",
    "    target=out,\n",
    "    wmin=-1,\n",
    "    wmax=1,\n",
    "    update_rule=MSTDP,\n",
    "    nu=1e-1,\n",
    "    norm=0.5 * middle.n,\n",
    ")\n",
    "\n",
    "# Add all layers and connections to the network.\n",
    "network.add_layer(inpt, name=\"Input Layer\")\n",
    "network.add_layer(middle, name=\"Hidden Layer\")\n",
    "network.add_layer(out, name=\"Output Layer\")\n",
    "network.add_connection(inpt_middle, source=\"Input Layer\", target=\"Hidden Layer\")\n",
    "network.add_connection(middle_out, source=\"Hidden Layer\", target=\"Output Layer\")\n",
    "\n",
    "# Load the Breakout environment.\n",
    "environment = GymEnvironment(\"CartPole-v0\")\n",
    "environment.reset()\n",
    "\n",
    "# Build pipeline from specified components.\n",
    "environment_pipeline = EnvironmentPipeline(\n",
    "    network,\n",
    "    environment,\n",
    "    encoding=poisson,\n",
    "    action_function=select_softmax,\n",
    "    output=\"Output Layer\",\n",
    "    time=100,\n",
    "    history_length=4,\n",
    "    delta=1,\n",
    "    plot_interval=1,\n",
    "    render_interval=1,\n",
    ")\n",
    "\n",
    "\n",
    "def run_pipeline(pipeline, episode_count):\n",
    "    ep_reward = []\n",
    "    for i in range(episode_count):\n",
    "        total_reward = 0\n",
    "        pipeline.reset_state_variables()\n",
    "        is_done = False\n",
    "        while not is_done:\n",
    "            result = pipeline.env_step()\n",
    "            pipeline.step(result)\n",
    "\n",
    "            reward = result[1]\n",
    "            total_reward += reward\n",
    "            is_done = result[2]\n",
    "        ep_reward+=[int(total_reward)]\n",
    "        print(f\"Episode {i} total reward:{total_reward}\")\n",
    "    return ep_reward\n",
    "\n",
    "\n",
    "print(\"Training: \")\n",
    "tr = run_pipeline(environment_pipeline, episode_count=10)\n",
    "print(tr)\n",
    "# stop MSTDP\n",
    "environment_pipeline.network.learning = False\n",
    "\n",
    "print(\"Testing: \")\n",
    "run_pipeline(environment_pipeline, episode_count=1)\n",
    "environment.generate_gif(str(1000)+\" Training Episodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bindsnet.encoding import bernoulli,poisson\n",
    "#from bindsnet.environment import GymEnvironment\n",
    "from bindsnet.learning import MSTDP\n",
    "from bindsnet.network import Network\n",
    "from bindsnet.network.nodes import Input, LIFNodes\n",
    "from bindsnet.network.topology import Connection\n",
    "from bindsnet.pipeline import EnvironmentPipeline\n",
    "from bindsnet.pipeline.action import select_softmax\n",
    "from collections import Counter\n",
    "# Build network.\n",
    "network = Network(dt=1.0)\n",
    "\n",
    "# Layers of neurons.\n",
    "inpt = Input(n=84 * 84 * 4 *4 , shape=[4, 1, 1, 4, 84, 84], traces=True)\n",
    "#inpt = Input(n=84 * 84 * 4, shape=[4,84,84], traces=True)\n",
    "#inpt = Input(n=80 * 80, shape=[1,1,4,84,84], traces=True)\n",
    "middle = LIFNodes(n=100, traces=True)\n",
    "out = LIFNodes(n=4, refrac=0, traces=True)\n",
    "\n",
    "# Connections between layers.\n",
    "inpt_middle = Connection(source=inpt,update_rule=MSTDP, target=middle, wmin=0, wmax=1e-1)\n",
    "middle_out = Connection(\n",
    "    source=middle,\n",
    "    target=out,\n",
    "    wmin=0,\n",
    "    wmax=1,\n",
    "    update_rule=MSTDP,\n",
    "    nu=1e-1,\n",
    "    norm=0.5 * middle.n,\n",
    ")\n",
    "\n",
    "# Add all layers and connections to the network.\n",
    "network.add_layer(inpt, name=\"Input Layer\")\n",
    "network.add_layer(middle, name=\"Hidden Layer\")\n",
    "network.add_layer(out, name=\"Output Layer\")\n",
    "network.add_connection(inpt_middle, source=\"Input Layer\", target=\"Hidden Layer\")\n",
    "network.add_connection(middle_out, source=\"Hidden Layer\", target=\"Output Layer\")\n",
    "\n",
    "# Load the Breakout environment.\n",
    "environment = GymEnvironment(\"BreakoutDeterministic-v4\")\n",
    "environment.reset()\n",
    "\n",
    "# Build pipeline from specified components.\n",
    "environment_pipeline = EnvironmentPipeline(\n",
    "    network,\n",
    "    environment,\n",
    "    encoding=poisson,\n",
    "    action_function=select_softmax,\n",
    "    output=\"Output Layer\",\n",
    "    time=100,\n",
    "    history_length=4,\n",
    "    delta=1,\n",
    "    plot_interval=1,\n",
    "    render_interval=1,\n",
    ")\n",
    "\n",
    "\n",
    "def run_pipeline(pipeline, episode_count):\n",
    "    ep_reward = []\n",
    "    for i in range(episode_count):\n",
    "        total_reward = 0\n",
    "        pipeline.reset_state_variables()\n",
    "        is_done = False\n",
    "        while not is_done:\n",
    "            result = pipeline.env_step()\n",
    "            pipeline.step(result)\n",
    "            reward = result[1]\n",
    "            total_reward += reward\n",
    "            is_done = result[2]\n",
    "        ep_reward+=[int(total_reward)]\n",
    "        print(f\"Episode {i} total reward:{total_reward}\")\n",
    "    return ep_reward\n",
    "\n",
    "\n",
    "print(\"Training: \")\n",
    "tr = run_pipeline(environment_pipeline, episode_count=10)\n",
    "print(tr)\n",
    "\n",
    "# stop MSTDP\n",
    "environment_pipeline.network.learning = False\n",
    "\n",
    "print(\"Testing: \")\n",
    "run_pipeline(environment_pipeline, episode_count=1)\n",
    "environment.generate_gif(str(1000)+\" Training Episodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(dict(Counter(tr)).keys(),dict(Counter(tr)).values())\n",
    "print(time.time()-ti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([i for i in range(len(tr))],tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(network.layers)\n",
    "print(network.connections)\n",
    "for l in network.connections.values():\n",
    "    print(l.w)\n",
    "environment.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.from_numpy(np.zeros((4,80,80)))\n",
    "\n",
    "x.masked_fill_(x != 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c43f0eb7841d2fd5ccf214152346b3b1b264169d58b5cbbfe51429b60e6e73a3"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
