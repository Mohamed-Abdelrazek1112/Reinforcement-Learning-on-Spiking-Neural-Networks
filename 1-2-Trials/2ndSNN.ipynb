{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import wrappers\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import random\n",
    "import time\n",
    "st = time.time()\n",
    "import cv2\n",
    "import pyNN.spiNNaker as p\n",
    "p.setup(timestep=20)\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inputFrameProcessor(frame):\n",
    "    return cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "def inputToSpikeRateArray(frame):\n",
    "    grayframe = inputFrameProcessor(frame) \n",
    "    grayframe= cv2.resize(grayframe,[84,84])\n",
    "    return [(m+1) for line in grayframe for m in line]\n",
    "def selectOutput(records,previous_spikes):\n",
    "    choice = 0\n",
    "    best = 0\n",
    "    j=-1\n",
    "    new_previous_spikes=[]\n",
    "    if previous_spikes==[]:\n",
    "        previous_spikes=[0]*len(records)\n",
    "    for rec in records:\n",
    "        j+=1\n",
    "        #print(\"action\",j,\"spikes\",len(rec)-previous_spikes[j])\n",
    "        new_previous_spikes.append(len(rec))\n",
    "    for i in range(len(records)):\n",
    "        if (len(records[i])-previous_spikes[i])>best:\n",
    "            best = len(records[i])-previous_spikes[i]\n",
    "            choice = i\n",
    "    return choice,new_previous_spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Pong-v0\")\n",
    "print(\"The environment has the following {} actions: {}\".format(env.action_space.n, \n",
    "                                                                env.unwrapped.get_action_meanings()))\n",
    "env = wrappers.Monitor(env, \"/tmp/Pong-v0\", force=True)\n",
    "print(len(env.unwrapped.get_action_meanings()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### INPUT LAYER #####\n",
    "inputLayer = p.Population(len(inputToSpikeRateArray(env.render(mode='rgb_array'))),p.SpikeSourcePoisson(rate=inputToSpikeRateArray(env.render(mode='rgb_array'))))\n",
    "inputLayer.record([\"spikes\"])\n",
    "##### Training INPUT ######\n",
    "if TRAIN:\n",
    "    rewardLayer = [p.Population(1,p.SpikeSourcePoisson(rate=1)) for i in range(env.action_space.n)]\n",
    "    punishmentLayer = [p.Population(1,p.SpikeSourcePoisson(rate=1)) for i in range(env.action_space.n)]\n",
    "    \n",
    "#####   STDP    #####\n",
    "timing_rule = p.SpikePairRule(tau_plus=0.1, tau_minus=0.1, A_plus=0.1, A_minus=0.1)\n",
    "weight_rule = p.AdditiveWeightDependence(w_max=10.0, w_min=0.01)\n",
    "stdp_model_excitatory = p.STDPMechanism(timing_dependence=timing_rule, weight_dependence=weight_rule, weight=5)\n",
    "#### SECOND LAYER ####\n",
    "pop=[p.Population(int(100),p.IF_curr_exp()) for action in range(env.action_space.n)]\n",
    "s = [m.record([\"spikes\"]) for m in pop]\n",
    "#### Projections ####\n",
    "projections01= [p.Projection(inputLayer,m,p.AllToAllConnector(), synapse_type=stdp_model_excitatory) for m in pop]\n",
    "rewardproj=[]\n",
    "punishmentproj = []\n",
    "for i in range(env.action_space.n):\n",
    "    rewardproj.append(p.Projection(rewardLayer[i],pop[i],p.AllToAllConnector(),synapse_type=p.StaticSynapse(weight=0.11)))\n",
    "    punishmentproj.append(p.Projection(punishmentLayer[i],pop[i],p.AllToAllConnector(), synapse_type=p.StaticSynapse(weight=-50)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "###RUN GAME TO CHECK IF ANYTHING IS ACHEIVED\n",
    "rwrd_running_tot=[]\n",
    "count=0\n",
    "for i in range(100):\n",
    "    step = 0\n",
    "    total_reward = 0\n",
    "    previous_spikes=[]\n",
    "    action=0\n",
    "    done =False\n",
    "    env.reset()\n",
    "    #for i in range(500):\n",
    "    #    action = random.randint(0,5)\n",
    "    #    env.step(action)\n",
    "    reward=0\n",
    "    #fig = plt.figure(figsize=(5, 5))\n",
    "    #img = plt.imshow(env.render(mode='rgb_array'))\n",
    "    while not done:\n",
    "        count+=1\n",
    "        print(count)\n",
    "        #img.set_data(env.render(mode='rgb_array'))\n",
    "        #display.display(plt.gcf())\n",
    "        #display.clear_output(wait=True)\n",
    "        #env.render()\n",
    "        ri = total_reward\n",
    "        \n",
    "        for i in range(10):\n",
    "            if not done:\n",
    "                observation, reward, done, info = env.step(action)\n",
    "        \n",
    "        total_reward+=reward\n",
    "        inputLayer.set(rate=inputToSpikeRateArray(observation))\n",
    "\n",
    "        #p.run_until(10*i)\n",
    "        #print(\"Action:############################################ \",action)\n",
    "        if reward > 0:#REWARD SIGNAL CHANGES WHEN REWARD CHANGES\n",
    "            for n in range(len(rewardLayer)):\n",
    "                if n==action:\n",
    "                    rewardLayer[n].set(rate = ((total_reward-ri)+1)*10)\n",
    "                else:\n",
    "                    rewardLayer[n].set(rate = 1)\n",
    "        else:\n",
    "            for n in range(len(punishmentLayer)):\n",
    "                if n==action:\n",
    "                    punishmentLayer[n].set(rate = (ri-total_reward)*10)\n",
    "                else:\n",
    "                    punishmentLayer[n].set(rate = 1)\n",
    "\n",
    "        total_reward += reward\n",
    "        action,previous_spikes = selectOutput([m.get_data(\"spikes\").segments[0].spiketrains[0] for m in pop],previous_spikes)\n",
    "        p.run_until(20*count)\n",
    "        #action,previous_spikes = selectOutput([m.get_data(\"spikes\").segments[0].spiketrains[0] for m in pop],previous_spikes)\n",
    "    print(\"GAME REWARD\",total_reward)\n",
    "    display.clear_output(wait=True)\n",
    "    #print(\"Total Reward\",total_reward)\n",
    "    rwrd_running_tot.append(total_reward)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot([i for i in range(len(rwrd_running_tot))],rwrd_running_tot)\n",
    "plt.savefig('plot.png', dpi=300, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rwrd_running_tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(time.time()-st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c43f0eb7841d2fd5ccf214152346b3b1b264169d58b5cbbfe51429b60e6e73a3"
  },
  "kernelspec": {
   "display_name": "sPyNNaker",
   "language": "python",
   "name": "spynnaker"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
