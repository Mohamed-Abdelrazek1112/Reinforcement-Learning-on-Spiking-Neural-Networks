{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989cd9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install opencv-python\n",
    "#%pip install torchvision\n",
    "#%pip install scikit-image\n",
    "#%pip install gym[Atari]\n",
    "#%pip install gym[accept-rom-license]\n",
    "#%pip install tensorflow\n",
    "#%pip install pyglet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17602ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import wrappers\n",
    "import tensorflow.compat.v1 as tf\n",
    "from skimage.transform import resize\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import random\n",
    "import time\n",
    "import imageio\n",
    "st = time.time()\n",
    "import cv2\n",
    "import pyNN.spiNNaker as p\n",
    "p.setup(timestep=1)\n",
    "import math\n",
    "import torchvision\n",
    "import numpy as np \n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77da362a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33da9ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inputToSpikeRateArray(frame):\n",
    "    if len(frame)>1:\n",
    "        return 0\n",
    "    frame = np.array(frame)\n",
    "    print(frame.flatten())\n",
    "    return frame.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d880cf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frameprocess(frame,frame_height=84, frame_width=65):\n",
    "    frame_height = frame_height\n",
    "    frame_width = frame_width\n",
    "    processed = tf.image.rgb_to_grayscale(frame)\n",
    "    processed = tf.image.crop_to_bounding_box(processed, 34, 0, 160, 140)\n",
    "    processed = tf.image.resize_images(processed, \n",
    "                                            [frame_height, frame_width], \n",
    "                                            method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd48a2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Atari(object):\n",
    "    \"\"\"Wrapper for the gym Atari to keep track of state\"\"\"\n",
    "    def __init__(self, envName, no_op_steps=10, agent_history_length=4):\n",
    "        self.env = gym.make(envName)\n",
    "        self.state = None\n",
    "        self.last_lives = 0\n",
    "        self.no_op_steps = no_op_steps\n",
    "        self.agent_history_length = agent_history_length\n",
    "\n",
    "    def reset(self,evaluation=False):\n",
    "        \"\"\"\n",
    "        Resets the environment and stacks four frames ontop of each other to \n",
    "        create the first state\n",
    "        \"\"\"\n",
    "        frame = self.env.reset()\n",
    "        self.last_lives = 0\n",
    "        terminal_life_lost = True # Set to true so that the agent starts \n",
    "                                  # with a 'FIRE' action when evaluating\n",
    "        if evaluation:\n",
    "            for _ in range(random.randint(1, self.no_op_steps)):\n",
    "                frame, _, _, _ = self.env.step(1) # Action 'Fire'\n",
    "        processed_frame = frameprocess(frame)\n",
    "        self.state = np.repeat(processed_frame, self.agent_history_length, axis=2)\n",
    "        \n",
    "        return terminal_life_lost\n",
    "\n",
    "    def step(self,action):\n",
    "        \"\"\"\n",
    "        Performs an action and observes the reward and terminal state from the environment\n",
    "        \"\"\"\n",
    "        new_frame, reward, terminal, info = self.env.step(action)\n",
    "            \n",
    "        if info['lives'] < self.last_lives:\n",
    "            terminal_life_lost = True\n",
    "        else:\n",
    "            terminal_life_lost = terminal\n",
    "        self.last_lives = info['lives']\n",
    "        \n",
    "        processed_new_frame = frameprocess(new_frame)\n",
    "        new_state = np.append(self.state[:, :, 1:], processed_new_frame, axis=2)\n",
    "        self.state = new_state\n",
    "        \n",
    "        return processed_new_frame, reward, terminal, terminal_life_lost, new_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1388b154",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_a_gif(frames_for_gif):\n",
    "    for idx, frame_idx in enumerate(frames_for_gif): \n",
    "        frames_for_gif[idx] = resize(frame_idx, (420, 320, 3), \n",
    "                                     preserve_range=True, order=0).astype(np.uint8)\n",
    "        \n",
    "    imageio.mimsave(\"ATARI_PONG.gif\", \n",
    "                    frames_for_gif, duration=1/30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c270ba47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def selectOutput(records,previous_spikes):\n",
    "    choice = 0\n",
    "    best = 0\n",
    "    j=-1\n",
    "    new_previous_spikes=[]\n",
    "    if previous_spikes==[]:\n",
    "        previous_spikes=[0]*len(records)\n",
    "    for rec in records:\n",
    "        j+=1\n",
    "        #print(\"action\",j,\"spikes\",len(rec)-previous_spikes[j])\n",
    "        new_previous_spikes.append(len(rec))\n",
    "    for i in range(len(records)):\n",
    "        if (len(records[i])-previous_spikes[i])>best:\n",
    "            best = len(records[i])-previous_spikes[i]\n",
    "            choice = i\n",
    "    return choice,new_previous_spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f5574b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Atari(\"PongDeterministic-v4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51c145b",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestep = 1.0\n",
    "duration = 3000\n",
    "\n",
    "# Main parameters from Izhikevich 2007 STDP paper\n",
    "#t_pre = [1500, 2400]  # Pre-synaptic neuron times\n",
    "#t_post = [1502]  # Post-synaptic neuron stimuli time\n",
    "#t_dopamine = [1600]  # Dopaminergic neuron spike times\n",
    "tau_c = 1000  # Eligibility trace decay time constant.\n",
    "tau_d = 200  # Dopamine trace decay time constant.\n",
    "DA_concentration_reward = [0.1]*env.env.action_space.n  # Dopamine trace step increase size\n",
    "DA_concentration_punishment = [0.1]*env.env.action_space.n\n",
    "# Initial weight\n",
    "rewarded_syn_weight = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b85013",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array((env.state)).size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d77a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "tau_c = 1000  # Eligibility trace decay time constant.\n",
    "tau_d = 200  # Dopamine trace decay time constant.\n",
    "DA_concentration = 0.1  # Dopamine trace step increase size\n",
    "\n",
    "\n",
    "##### INPUT LAYER #####\n",
    "inputLayer = p.Population(np.array((env.state)).size,p.SpikeSourcePoisson(rate=inputToSpikeRateArray(env.state)))\n",
    "inputLayer.record([\"spikes\"])\n",
    "##### Training INPUT ######\n",
    "if TRAIN:\n",
    "    rewardLayer = [p.Population(1,p.SpikeSourcePoisson(rate=1)) for i in range(env.env.action_space.n)]\n",
    "    punishmentLayer = [p.Population(1,p.SpikeSourcePoisson(rate=1)) for i in range(env.env.action_space.n)]\n",
    "    \n",
    "#####   STDP    #####\n",
    "timing_rule = p.SpikePairRule(tau_plus=0.1, tau_minus=0.1, A_plus=0.1, A_minus=0.1)\n",
    "weight_rule = p.AdditiveWeightDependence(w_max=10.0, w_min=0.01)\n",
    "stdp_model_excitatory = p.STDPMechanism(timing_dependence=timing_rule, weight_dependence=weight_rule, weight=5)\n",
    "##### NMSTDP ######\n",
    "\n",
    "\n",
    "#### SECOND LAYER ####\n",
    "pop=[p.Population(int(100),p.IF_curr_exp()) for action in range(env.env.action_space.n)]\n",
    "s = [m.record([\"spikes\"]) for m in pop]\n",
    "\n",
    "#### Projections ####\n",
    "projections01= [p.Projection(inputLayer,m,p.AllToAllConnector(), synapse_type=stdp_model_excitatory) for m in pop]\n",
    "rewardproj=[]\n",
    "punishmentproj = []\n",
    "for i in range(env.env.action_space.n):\n",
    "    rewardproj.append(p.Projection(\n",
    "    rewardLayer[i],pop[i],\n",
    "    p.AllToAllConnector(),\n",
    "    synapse_type=p.extra_models.Neuromodulation(\n",
    "    weight=DA_concentration_reward[i], tau_c=tau_c, tau_d=tau_d, w_max=20.0),\n",
    "receptor_type='reward', label='reward synapses'))\n",
    "                      \n",
    "    punishmentproj.append(p.Projection(\n",
    "    punishmentLayer[i],pop[i],\n",
    "    p.AllToAllConnector(),\n",
    "    synapse_type=p.extra_models.Neuromodulation(\n",
    "    weight=DA_concentration_punishment[i], tau_c=tau_c, tau_d=tau_d, w_max=20.0),\n",
    "receptor_type='reward', label='reward synapses'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e482c616",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Atari(\"PongDeterministic-v4\")\n",
    "env.reset()\n",
    "frames=[]\n",
    "total_reward=0\n",
    "done = False\n",
    "count=0\n",
    "previous_spikes=[]\n",
    "rwrd_running_tot=[]\n",
    "while not done:\n",
    "    #observation, reward, done, info,new_frame = env.step(action)\n",
    "    env.env.render()\n",
    "    count+=1\n",
    "    print(count)\n",
    "    ri = total_reward\n",
    "\n",
    "    action,previous_spikes = selectOutput([m.get_data(\"spikes\").segments[0].spiketrains[0] for m in pop],previous_spikes)\n",
    "    for i in range(10):\n",
    "        observation, reward, done, info,new = env.step(action)#processed_new_frame, reward, terminal, terminal_life_lost, new_frame\n",
    "    \n",
    "    total_reward+=reward\n",
    "    inputLayer.set(rate=inputToSpikeRateArray(env.state))\n",
    "\n",
    "    if reward > 0:#REWARD SIGNAL CHANGES WHEN REWARD CHANGES\n",
    "        DA_concentration_reward[action] *= (reward+1)\n",
    "    else:\n",
    "        DA_concentration_punishment[action] *= 1/abs(reward+1)\n",
    "\n",
    "    total_reward += reward\n",
    "    p.run(1)\n",
    "\n",
    "rwrd_running_tot.append(total_reward)\n",
    "print(\"GAME REWARD\",total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3226ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot([i for i in range(len(rwrd_running_tot))],rwrd_running_tot)\n",
    "plt.savefig('plot.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7039151c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rwrd_running_tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd88c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(time.time()-st)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sPyNNakerGit",
   "language": "python",
   "name": "spynnakergit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
