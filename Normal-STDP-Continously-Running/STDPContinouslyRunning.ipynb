{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all the parts needed\n",
    "#%pip install gym pyvirtualdisplay matplotlib\n",
    "#%pip install gym[atari]\n",
    "#%pip install gym[accept-rom-license]\n",
    "#%pip install sklearn\n",
    "#%pip install scikit-image\n",
    "#%pip install torchvision\n",
    "#%pip install tensorflow -qq\n",
    "import cv2\n",
    "import imageio\n",
    "import numpy as np\n",
    "from skimage.transform import resize\n",
    "#import torch.nn.functional as F\n",
    "import random\n",
    "import torchvision.transforms as transforms\n",
    "import tensorflow.compat.v1 as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frameprocess(frame,frame_height=84, frame_width=65):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        frame_height: Integer, Height of a frame of an Atari game\n",
    "        frame_width: Integer, Width of a frame of an Atari game\n",
    "    \"\"\"\n",
    "    frame_height = frame_height\n",
    "    frame_width = frame_width\n",
    "    processed = tf.image.rgb_to_grayscale(frame)\n",
    "    processed = tf.image.crop_to_bounding_box(processed, 34, 0, 160, 140)\n",
    "    processed = tf.image.resize_images(processed, \n",
    "                                            [frame_height, frame_width], \n",
    "                                            method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selectOutput(records,previous_spikes):\n",
    "    choice = 0\n",
    "    best = 0\n",
    "    j=-1\n",
    "    new_previous_spikes=[]\n",
    "    if previous_spikes==[]:\n",
    "        previous_spikes=[0]*len(records)\n",
    "    for rec in records:\n",
    "        j+=1\n",
    "        #print(\"action\",j,\"spikes\",len(rec)-previous_spikes[j])\n",
    "        new_previous_spikes.append(len(rec))\n",
    "    for i in range(len(records)):\n",
    "        if (len(records[i])-previous_spikes[i]) > best:\n",
    "            best = len(records[i])-previous_spikes[i]\n",
    "            choice = i\n",
    "    return choice,new_previous_spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Atari(object):\n",
    "    \"\"\"Wrapper for the environment provided by gym\"\"\"\n",
    "    def __init__(self, envName, no_op_steps=10, agent_history_length=4):\n",
    "        self.env = gym.make(envName)\n",
    "        self.state = None\n",
    "        self.last_lives = 0\n",
    "        self.no_op_steps = no_op_steps\n",
    "        self.metadata = self.env.metadata\n",
    "        self.agent_history_length = agent_history_length\n",
    "        self.spec = self.env.spec\n",
    "        self.render = self.env.render\n",
    "\n",
    "    def reset(self,evaluation=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sess: A Tensorflow session object\n",
    "            evaluation: A boolean saying whether the agent is evaluating or training\n",
    "        Resets the environment and stacks four frames ontop of each other to \n",
    "        create the first state\n",
    "        \"\"\"\n",
    "        frame = self.env.reset()\n",
    "        self.last_lives = 0\n",
    "        terminal_life_lost = True # Set to true so that the agent starts \n",
    "                                  # with a 'FIRE' action when evaluating\n",
    "        if evaluation:\n",
    "            for _ in range(random.randint(1, self.no_op_steps)):\n",
    "                frame, _, _, _ = self.env.step(1) # Action 'Fire'\n",
    "        processed_frame = frameprocess(frame)   # (★★★)\n",
    "        self.state = np.repeat(processed_frame, self.agent_history_length, axis=2)\n",
    "        \n",
    "        return terminal_life_lost\n",
    "\n",
    "    def step(self,action):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            action: Integer, action the agent performs\n",
    "        Performs an action and observes the reward and terminal state from the environment\n",
    "        \"\"\"\n",
    "        new_frame, reward, done, info = self.env.step(action)  # (5★)\n",
    "            \n",
    "        if info['lives'] < self.last_lives:\n",
    "            terminal_life_lost = True\n",
    "        else:\n",
    "            terminal_life_lost = done\n",
    "        self.last_lives = info['lives']\n",
    "        \n",
    "        processed_new_frame = frameprocess(new_frame)   # (6★)\n",
    "        new_state = np.append(self.state[:, :, 1:], processed_new_frame, axis=2) # (6★)   \n",
    "        self.state = new_state\n",
    "        \n",
    "        return processed_new_frame, reward, done, new_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def inputToSpikeRateArray(frame):\n",
    "    f = np.array(frame)\n",
    "    return f.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyNN.spiNNaker as p\n",
    "def run_spinnaker_sim(input_label,reward_input_label,punishment_input_label, output_label, connection_port):\n",
    "    tau_c = 1000  # Eligibility trace decay time constant.\n",
    "    tau_d = 200  # Dopamine trace decay time constant.\n",
    "      # Dopamine trace step increase size\n",
    "    inputpopsize = 84*84\n",
    "    # Set up the simulation itself\n",
    "    p.setup(1.0)\n",
    "\n",
    "    ##### INPUT LAYER #####\n",
    "    input_pop = p.Population(inputpopsize,p.external_devices.SpikeInjector(\n",
    "            database_notify_port_num=connection_port),label = input_label)\n",
    "    \n",
    "    reward_pop = [p.Population(10,p.external_devices.SpikeInjector(database_notify_port_num=connection_port),label = reward_input_label) for _ in range(actions)]\n",
    "    \n",
    "    punishment_pop = [p.Population(10,p.external_devices.SpikeInjector(database_notify_port_num=connection_port),label = punishment_input_label) for _ in range(actions)]\n",
    "    \n",
    "\n",
    "    #####   STDP    #####\n",
    "    timing_rule = p.SpikePairRule(tau_plus=0.1, tau_minus=0.1, A_plus=0.1, A_minus=0.1)\n",
    "    weight_rule = p.AdditiveWeightDependence(w_max=10.0, w_min=0.01)\n",
    "    stdp_model_excitatory = p.STDPMechanism(timing_dependence=timing_rule, weight_dependence=weight_rule, weight=5)\n",
    "    \n",
    "    #### OUTPUT LAYER ####\n",
    "    output_pop = [p.Population(int(100),p.IF_curr_exp(),label = output_label) for action in range(4)]\n",
    "    \n",
    "    ## Create synaptic connections\n",
    "    # ALL INPUTS CONNECT TO ALL OUTPUTS\n",
    "    inoutsyn = []\n",
    "    rewardoutsyn = []\n",
    "    punishmentoutsyn = []\n",
    "    k=0\n",
    "    for pop in output_pop: \n",
    "        inoutsyn+=[p.Projection(input_pop,pop,p.AllToAllConnector(), synapse_type=stdp_model_excitatory)]\n",
    "        rewardoutsyn+=[p.Projection(reward_pop[k],pop,p.AllToAllConnector(), synapse_type=stdp_model_excitatory)]\n",
    "        punishmentoutsyn+=[p.Projection(punishment_pop[k],pop,p.AllToAllConnector(), synapse_type=stdp_model_excitatory)]\n",
    "        k+=1\n",
    "\n",
    "    \n",
    "    # Make the population output spikes\n",
    "    for op in output_pop:\n",
    "        p.external_devices.activate_live_output_for(\n",
    "            op, database_notify_port_num=connection_port)\n",
    "\n",
    "    # Run in sections of 20ms\n",
    "    p.external_devices.run_forever(sync_time=20)\n",
    "    \n",
    "    # End the simulation once complete (run_forever stops when requested elsewhere)\n",
    "    p.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import wrappers\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import pyNN.spiNNaker as p\n",
    "\n",
    "\n",
    "# Note the odd order of parameters here is because input_label and connection\n",
    "# are from the default interface, but output_label and spike_queue are from\n",
    "# additional parameters\n",
    "def run_openai_gym_sim(input_label,reward_input_label,punishment_input_label,connection, output_label, spike_queue, new_spike_queue, plt, img, env):\n",
    "    print(\"yuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuu\")\n",
    "    print(env.state)\n",
    "    observation = env.reset()\n",
    "    step = 0\n",
    "    total_reward = 0\n",
    "    previous_spikes = []\n",
    "    while True:\n",
    "        step += 1\n",
    "        action_leading_to_reward = []\n",
    "        action_leading_to_punishment = []\n",
    "        # Display the simulation at the start of the step\n",
    "        img.set_data(env.render(mode='rgb_array'))\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n",
    "\n",
    "        action,previous_spikes = selectOutput(new_spike_queue,previous_spikes)\n",
    "        \n",
    "        # TODO: Pull from the spike queue and use them to \"decide the action\"\n",
    "        while spike_queue:\n",
    "            label, time, neuron_ids = spike_queue.pop()\n",
    "        \n",
    "        # Move the simulation forwards\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if reward>0:\n",
    "            action_leading_to_reward.append(action)\n",
    "        elif reward<0:\n",
    "            action_leading_to_punishment.append(action)\n",
    "            \n",
    "        total_reward += reward\n",
    "        \n",
    "        # If finished, stop\n",
    "        if done:\n",
    "            \n",
    "            print(f\"Steps: {step}, score: {total_reward}\")\n",
    "            break\n",
    "        \n",
    "        # TODO: Use the output from the environment to send spikes to SpiNNaker\n",
    "        connection.send_spikes(input_label,inputToSpikeRateArray(env.state))#SPIKES TO BE SENT)\n",
    "        connection.send_spikes(reward_input_label,[1]*len(action_leading_to_reward))\n",
    "        \n",
    "        # Continue spinnaker simulation for next run of loop\n",
    "        p.external_devices.continue_simulation()\n",
    "        \n",
    "    # Stop the OpenAI Gym\n",
    "    env.close()\n",
    "    \n",
    "    # Stop SpiNNaker\n",
    "    p.external_devices.request_stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pulator\n",
    "import pyNN.spiNNaker as p\n",
    "from functools import partial\n",
    "from collections import deque\n",
    "\n",
    "# A queue of spikes\n",
    "spike_queue = deque()\n",
    "new_spike_queue = []\n",
    "# A function to receive spikes and put them in a queue\n",
    "def receive_spikes(label, time, neuron_ids):\n",
    "    spike_queue.appendleft((label, time, neuron_ids))\n",
    "    new_spike_queue = []\n",
    "    new_spike_queue.append((label, time, neuron_ids))\n",
    "\n",
    "# Keep track of the labels these need to match up in several places\n",
    "input_label = \"input\"\n",
    "output_label = \"output\"\n",
    "reward_input_label = \"reward\"\n",
    "punishment_input_label = \"punishment\"\n",
    "\n",
    "# Create the connection.\n",
    "# Note the use of local_port=None allows the automatic assignment of a port.\n",
    "connection = p.external_devices.SpynnakerLiveSpikesConnection(\n",
    "    local_port=None, send_labels=[input_label,reward_input_label], receive_labels=[output_label])\n",
    "\n",
    "# Make the OpenAI Gym simulation\n",
    "env = Atari(\"Breakout-v0\")\n",
    "env = wrappers.Monitor(env, \"/tmp/Breakout-v0\", force=True)\n",
    "\n",
    "# Display Breakout-v0 = plt.figure(figsize=(5, 5))\n",
    "img = plt.imshow(env.env.render(mode='rgb_array'))\n",
    "\n",
    "# Register the OpenAI Gym function to be called when the simulation starts\n",
    "# Note: we have to use a label here so we chose input_label\n",
    "#connection.add_start_resume_callback(input_label,partial(run_openai_gym_sim,punishment_input_label=punishment_input_label,connection=connection,output_label=output_label, spike_queue=spike_queue, new_spike_queue=new_spike_queue, plt=plt, img=img, env=env))\n",
    "\n",
    "connection.add_start_resume_callback(reward_input_label,partial(run_openai_gym_sim,punishment_input_label=punishment_input_label,connection=connection,output_label=output_label, spike_queue=spike_queue, new_spike_queue=new_spike_queue, plt=plt, img=img, env=env))\n",
    "# Register the receive spikes function\n",
    "connection.add_receive_callback(output_label, receive_spikes)\n",
    "\n",
    "# Run the simulation\n",
    "run_spinnaker_sim(input_label,reward_input_label,punishment_input_label, output_label, connection.local_port)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sPyNNaker",
   "language": "python",
   "name": "spynnaker"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
